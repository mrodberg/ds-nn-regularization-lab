{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll gain experience detecting and dealing with a ANN model that is overfitting using various regularization and hyperparameter tuning techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this lab, we'll work with a large dataset of customer complaints to a bank, with the goal of predicting what product the customer is complaining about based on the text of their complaint.  There are 7 different possible products that we can predict, making this a multi-class classification task. \n",
    "\n",
    "\n",
    "#### Preprocessing our Data Set\n",
    "We'll start by preprocessing our dataset by tokenizing the complaints and limiting the number of words we consider to reduce dimensionality. \n",
    "\n",
    "#### Building our Tuning our Model\n",
    "Once we have preprocessed our data set, we'll build a model and explore the various ways that we can reduce overfitting using the following strategies:\n",
    "- Early stopping to minimize the discrepancy between train and test accuracy.\n",
    "- L1 and L2 regularization.\n",
    "- Dropout regularization.\n",
    "- Using more data.\n",
    "\n",
    "\n",
    "**_Let's Get Started!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Bank Complaints Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the libraries and take a sample\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the cell below, import our data into a DataFrame.  The data is currently stored in `Bank_complaints.csv`.\n",
    "Then, `.describe()` the dataset to get a feel for what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>59724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am filing this complaint because Experian ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11404</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Product                       Consumer complaint narrative\n",
       "count          60000                                              60000\n",
       "unique             7                                              59724\n",
       "top     Student loan  I am filing this complaint because Experian ha...\n",
       "freq           11404                                                 26"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed things up during the development process (and also to give us the ability to see how adding more data affects our model performance), we're going to work with a sample of our dataset rather than the whole thing.  The entire dataset consists of 60,000 rows--we're going to build a model using only 10,000 items randomly sampled from this.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Get a random sample of `10000` items from our dataset (HINT: use the `df` object's `.sample()` method to make this easy)\n",
    "* Reset the indexes on these samples to `range(10000)`, so that the indices for our rows are sequential and make sense.\n",
    "* Store our labels, which are found in `\"Product\"`, in a different variable.\n",
    "* Store the data, found in `\"Consumer complaint narrative`, in the variable `complaints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenizing the Complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only keep 2,000 most common words and use one-hot encoding to quickly vectorize our dataset from text into a format that a neural network can work with. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `Tokenizer()` object, and set the `num_words` parameter to `2000`.\n",
    "* Call the tokenizer object's `fit_on_texts()` method and pass in our `complaints` variable we created above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create some text sequences by calling the `tokenizer` object's `.texts_to_sequences()` method and feeding in our `complaints` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll convert our text data from text to a vectorized matrix.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Call the `tokenizer` object's `.texts_to_matrix` method, passing in our `complaints` variable, as well as setting the `mode` parameter equal to `'binary'`.\n",
    "* Store the tokenizer's `.word_index` in the appropriate variable.\n",
    "* Check the `np.shape()` of our `one_hot_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results) # Expected Results (10000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One-hot Encoding of the Products Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized and encoded our text data, we still need to one-hot encode our label data.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "\n",
    "* Create a `LabelEncoder` object, which can found inside the `preprocessing` module.\n",
    "* `fit` the label encoder we just created to `product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what classes our label encoder found.  Run the cell below to examine a list of classes that `product` contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank account or service',\n",
       " 'Checking or savings account',\n",
       " 'Consumer Loan',\n",
       " 'Credit card',\n",
       " 'Credit reporting',\n",
       " 'Mortgage',\n",
       " 'Student loan']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to transform `product` into a numeric vector.  \n",
    "\n",
    "In the cell below, use the label encoder's `.transform` method on `product` to create an integer encoded version of our labels. \n",
    "\n",
    "Then, access `product_cat` to see an example of how the labels are now encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 6, 5, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cat = le.transform(product) \n",
    "product_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to go from integer encoding to one-hot encoding.  Use the `to_categorical` method from keras to do this easily in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the shape of our one-hot encoded labels to make sure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(product_onehot) # Expected Output: (10000, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train - test split\n",
    "\n",
    "Now, we'll split our data into training and testing sets.  \n",
    "\n",
    "\n",
    "We'll accomplish this by generating a random list of 1500 different indices between 1 and 10000.  Then, we'll slice these rows and store them as our test set, and delete them from the training set (it's very important to remember to remove them from the training set!)\n",
    "\n",
    "Run the cell below to create a set of random indices for our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = random.sample(range(1,10000), 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "\n",
    "* Slice the `test_index` rows from `one_hot_results` and store them in `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = one_hot_results[test_index]\n",
    "\n",
    "# This line returns a version of our one_hot_results that has every item with an index in test_index removed\n",
    "train = np.delete(one_hot_results, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to repeat the splitting process on our labels, making sure that we use the same indices we used to split our data. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Slice `test_index` from `product_onehot`\n",
    "* Use `np.delete` to remove `test_index` items from `product_onehot` (the syntax is exactly the same above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the shape everything we just did to make sure that the dimensions match up.  \n",
    "\n",
    "In the cell below, use `np.shape` to check the shape of:\n",
    "\n",
    "* `label_test`\n",
    "* `label_train`\n",
    "* `test`\n",
    "* `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 7)\n",
      "(8500, 7)\n",
      "(1500, 2000)\n",
      "(8500, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(label_test)) # Expected Output: (1500, 7)\n",
    "print(np.shape(label_train)) # Expected Output: (8500, 7)\n",
    "print(np.shape(test)) # Expected Output: (1500, 2000)\n",
    "print(np.shape(train)) # Expected Output: (8500, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`.\n",
    "\n",
    "Run the cell below to create our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "val = train[:1000]\n",
    "train_final = train[1000:]\n",
    "label_val = label_train[:1000]\n",
    "label_train_final = label_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating, compiling and running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Sequential` from the appropriate module in keras.\n",
    "* Import `Dense` from the appropriate module in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a model with the following specifications in the cell below:\n",
    "\n",
    "* An input layer of shape `(2000,)`\n",
    "* Hidden layer 1: Dense, 50 neurons, relu activation \n",
    "* Hidden layer 2: Dense, 25 neurons, relu activation\n",
    "* Output layer: Dense, 7 neurons, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, `compile` the model with the following settings:\n",
    "\n",
    "* Optimizer is `\"SGD\"`\n",
    "* Loss is `'categorical_crossentropy'`\n",
    "* metrics is `['accuracy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Train the model for 120 epochs in mini-batches of 256 samples. Also pass in `(val, label_val)` to the `validation_data` parameter, so that we see how our model does on the test set after every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.9547 - accuracy: 0.1440 - val_loss: 1.9305 - val_accuracy: 0.2000\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.9210 - accuracy: 0.1960 - val_loss: 1.9014 - val_accuracy: 0.2470\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8945 - accuracy: 0.2368 - val_loss: 1.8746 - val_accuracy: 0.2780\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8677 - accuracy: 0.2624 - val_loss: 1.8467 - val_accuracy: 0.2980\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8389 - accuracy: 0.2896 - val_loss: 1.8155 - val_accuracy: 0.3130\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8067 - accuracy: 0.3127 - val_loss: 1.7815 - val_accuracy: 0.3350\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.7704 - accuracy: 0.3369 - val_loss: 1.7438 - val_accuracy: 0.3530\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7300 - accuracy: 0.3611 - val_loss: 1.7034 - val_accuracy: 0.3670\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6864 - accuracy: 0.3829 - val_loss: 1.6602 - val_accuracy: 0.3820\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6404 - accuracy: 0.4012 - val_loss: 1.6149 - val_accuracy: 0.4040\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5932 - accuracy: 0.4227 - val_loss: 1.5697 - val_accuracy: 0.4260\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5454 - accuracy: 0.4448 - val_loss: 1.5234 - val_accuracy: 0.4490\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4972 - accuracy: 0.4655 - val_loss: 1.4792 - val_accuracy: 0.4700\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4501 - accuracy: 0.4893 - val_loss: 1.4347 - val_accuracy: 0.5070\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4036 - accuracy: 0.5131 - val_loss: 1.3920 - val_accuracy: 0.5320\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3583 - accuracy: 0.5345 - val_loss: 1.3500 - val_accuracy: 0.5530\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3144 - accuracy: 0.5611 - val_loss: 1.3090 - val_accuracy: 0.5810\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.2712 - accuracy: 0.5844 - val_loss: 1.2700 - val_accuracy: 0.5850\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.2303 - accuracy: 0.6037 - val_loss: 1.2320 - val_accuracy: 0.6060\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1898 - accuracy: 0.6205 - val_loss: 1.1947 - val_accuracy: 0.6320\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1509 - accuracy: 0.6411 - val_loss: 1.1583 - val_accuracy: 0.6380\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1137 - accuracy: 0.6521 - val_loss: 1.1237 - val_accuracy: 0.6530\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0782 - accuracy: 0.6673 - val_loss: 1.0932 - val_accuracy: 0.6580\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0437 - accuracy: 0.6775 - val_loss: 1.0606 - val_accuracy: 0.6630\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0119 - accuracy: 0.6891 - val_loss: 1.0322 - val_accuracy: 0.6740\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9813 - accuracy: 0.6953 - val_loss: 1.0065 - val_accuracy: 0.6840\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9529 - accuracy: 0.7035 - val_loss: 0.9827 - val_accuracy: 0.6950\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9265 - accuracy: 0.7104 - val_loss: 0.9582 - val_accuracy: 0.6950\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9012 - accuracy: 0.7176 - val_loss: 0.9379 - val_accuracy: 0.6940\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8780 - accuracy: 0.7241 - val_loss: 0.9183 - val_accuracy: 0.6990\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8561 - accuracy: 0.7313 - val_loss: 0.8979 - val_accuracy: 0.7050\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8356 - accuracy: 0.7347 - val_loss: 0.8819 - val_accuracy: 0.7070\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.8168 - accuracy: 0.7380 - val_loss: 0.8687 - val_accuracy: 0.7120\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7986 - accuracy: 0.7407 - val_loss: 0.8524 - val_accuracy: 0.7100\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7815 - accuracy: 0.7424 - val_loss: 0.8376 - val_accuracy: 0.7150\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7658 - accuracy: 0.7468 - val_loss: 0.8247 - val_accuracy: 0.7200\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7509 - accuracy: 0.7509 - val_loss: 0.8142 - val_accuracy: 0.7250\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7374 - accuracy: 0.7527 - val_loss: 0.8019 - val_accuracy: 0.7290\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7241 - accuracy: 0.7555 - val_loss: 0.7921 - val_accuracy: 0.7240\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7121 - accuracy: 0.7603 - val_loss: 0.7831 - val_accuracy: 0.7290\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7003 - accuracy: 0.7623 - val_loss: 0.7740 - val_accuracy: 0.7270\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.7636 - val_loss: 0.7664 - val_accuracy: 0.7290\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6789 - accuracy: 0.7671 - val_loss: 0.7616 - val_accuracy: 0.7310\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6688 - accuracy: 0.7679 - val_loss: 0.7525 - val_accuracy: 0.7360\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6597 - accuracy: 0.7705 - val_loss: 0.7444 - val_accuracy: 0.7440\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6507 - accuracy: 0.7737 - val_loss: 0.7386 - val_accuracy: 0.7430\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6418 - accuracy: 0.7769 - val_loss: 0.7329 - val_accuracy: 0.7460\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6341 - accuracy: 0.7789 - val_loss: 0.7292 - val_accuracy: 0.7370\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6259 - accuracy: 0.7800 - val_loss: 0.7226 - val_accuracy: 0.7430\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6188 - accuracy: 0.7811 - val_loss: 0.7180 - val_accuracy: 0.7490\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6111 - accuracy: 0.7855 - val_loss: 0.7164 - val_accuracy: 0.7450\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6046 - accuracy: 0.7848 - val_loss: 0.7113 - val_accuracy: 0.7430\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5976 - accuracy: 0.7880 - val_loss: 0.7069 - val_accuracy: 0.7390\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5910 - accuracy: 0.7883 - val_loss: 0.7026 - val_accuracy: 0.7460\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5851 - accuracy: 0.7919 - val_loss: 0.7000 - val_accuracy: 0.7370\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5792 - accuracy: 0.7907 - val_loss: 0.6944 - val_accuracy: 0.7490\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.5713 - accuracy: 0.79 - 0s 6ms/step - loss: 0.5729 - accuracy: 0.7964 - val_loss: 0.6918 - val_accuracy: 0.7440\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5676 - accuracy: 0.7963 - val_loss: 0.6897 - val_accuracy: 0.7350\n",
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5623 - accuracy: 0.7941 - val_loss: 0.6840 - val_accuracy: 0.7450\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5568 - accuracy: 0.8015 - val_loss: 0.6841 - val_accuracy: 0.7470\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5514 - accuracy: 0.8011 - val_loss: 0.6819 - val_accuracy: 0.7480\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5465 - accuracy: 0.8056 - val_loss: 0.6792 - val_accuracy: 0.7490\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5414 - accuracy: 0.8063 - val_loss: 0.6766 - val_accuracy: 0.7460\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5369 - accuracy: 0.8088 - val_loss: 0.6775 - val_accuracy: 0.7450\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5322 - accuracy: 0.8107 - val_loss: 0.6719 - val_accuracy: 0.7490\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5274 - accuracy: 0.8116 - val_loss: 0.6709 - val_accuracy: 0.7490\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5227 - accuracy: 0.8135 - val_loss: 0.6707 - val_accuracy: 0.7490\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5189 - accuracy: 0.8153 - val_loss: 0.6683 - val_accuracy: 0.7500\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5144 - accuracy: 0.8173 - val_loss: 0.6647 - val_accuracy: 0.7520\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5099 - accuracy: 0.8179 - val_loss: 0.6662 - val_accuracy: 0.7510\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5057 - accuracy: 0.8179 - val_loss: 0.6627 - val_accuracy: 0.7530\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5014 - accuracy: 0.8201 - val_loss: 0.6612 - val_accuracy: 0.7530\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4978 - accuracy: 0.8219 - val_loss: 0.6604 - val_accuracy: 0.7560\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4937 - accuracy: 0.8228 - val_loss: 0.6580 - val_accuracy: 0.7560\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4902 - accuracy: 0.8241 - val_loss: 0.6570 - val_accuracy: 0.7520\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4859 - accuracy: 0.8267 - val_loss: 0.6586 - val_accuracy: 0.7490\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4824 - accuracy: 0.8279 - val_loss: 0.6547 - val_accuracy: 0.7570\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4780 - accuracy: 0.8293 - val_loss: 0.6540 - val_accuracy: 0.7610\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4747 - accuracy: 0.8323 - val_loss: 0.6537 - val_accuracy: 0.7540\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4713 - accuracy: 0.8324 - val_loss: 0.6546 - val_accuracy: 0.7540\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.4675 - accuracy: 0.8309 - val_loss: 0.6523 - val_accuracy: 0.7610\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.8344 - val_loss: 0.6523 - val_accuracy: 0.7540\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4604 - accuracy: 0.8364 - val_loss: 0.6489 - val_accuracy: 0.7560\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4569 - accuracy: 0.8393 - val_loss: 0.6495 - val_accuracy: 0.7600\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.4537 - accuracy: 0.8400 - val_loss: 0.6484 - val_accuracy: 0.7580\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4500 - accuracy: 0.8395 - val_loss: 0.6521 - val_accuracy: 0.7560\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4473 - accuracy: 0.8405 - val_loss: 0.6478 - val_accuracy: 0.7550\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4434 - accuracy: 0.8425 - val_loss: 0.6464 - val_accuracy: 0.7570\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4403 - accuracy: 0.8451 - val_loss: 0.6459 - val_accuracy: 0.7570\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4371 - accuracy: 0.8475 - val_loss: 0.6451 - val_accuracy: 0.7570\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.4339 - accuracy: 0.8476 - val_loss: 0.6460 - val_accuracy: 0.7590\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4308 - accuracy: 0.8483 - val_loss: 0.6454 - val_accuracy: 0.7590\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4276 - accuracy: 0.8505 - val_loss: 0.6470 - val_accuracy: 0.7630\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4246 - accuracy: 0.8524 - val_loss: 0.6423 - val_accuracy: 0.7630\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4212 - accuracy: 0.8532 - val_loss: 0.6432 - val_accuracy: 0.7680\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.4183 - accuracy: 0.8532 - val_loss: 0.6413 - val_accuracy: 0.7590\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4156 - accuracy: 0.8545 - val_loss: 0.6431 - val_accuracy: 0.7590\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.4125 - accuracy: 0.8564 - val_loss: 0.6409 - val_accuracy: 0.7640\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.4096 - accuracy: 0.8565 - val_loss: 0.6399 - val_accuracy: 0.7630\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4069 - accuracy: 0.8577 - val_loss: 0.6409 - val_accuracy: 0.7650\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4036 - accuracy: 0.8581 - val_loss: 0.6437 - val_accuracy: 0.7630\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4010 - accuracy: 0.8623 - val_loss: 0.6427 - val_accuracy: 0.7660\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3982 - accuracy: 0.8617 - val_loss: 0.6397 - val_accuracy: 0.7650\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3955 - accuracy: 0.8635 - val_loss: 0.6407 - val_accuracy: 0.7680\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3928 - accuracy: 0.8637 - val_loss: 0.6422 - val_accuracy: 0.7710\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3898 - accuracy: 0.8657 - val_loss: 0.6407 - val_accuracy: 0.7590\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3867 - accuracy: 0.8680 - val_loss: 0.6440 - val_accuracy: 0.7540\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.3847 - accuracy: 0.8695 - val_loss: 0.6433 - val_accuracy: 0.7610\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3819 - accuracy: 0.8711 - val_loss: 0.6397 - val_accuracy: 0.7690\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.3791 - accuracy: 0.8703 - val_loss: 0.6372 - val_accuracy: 0.7650\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.3763 - accuracy: 0.8715 - val_loss: 0.6417 - val_accuracy: 0.7730\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3737 - accuracy: 0.8731 - val_loss: 0.6389 - val_accuracy: 0.7670\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3713 - accuracy: 0.8744 - val_loss: 0.6414 - val_accuracy: 0.7720\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.3687 - accuracy: 0.8744 - val_loss: 0.6416 - val_accuracy: 0.7720\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.3662 - accuracy: 0.8760 - val_loss: 0.6384 - val_accuracy: 0.7710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3635 - accuracy: 0.8773 - val_loss: 0.6394 - val_accuracy: 0.7710\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.3606 - accuracy: 0.8773 - val_loss: 0.6392 - val_accuracy: 0.7730\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3585 - accuracy: 0.8803 - val_loss: 0.6384 - val_accuracy: 0.7760\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8801 - val_loss: 0.6399 - val_accuracy: 0.7720\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3535 - accuracy: 0.8801 - val_loss: 0.6387 - val_accuracy: 0.7720\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Store the model's `.history` inside of `model_val_dict`\n",
    "* Check what `keys()` this dictionary contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the final results on the training and testing sets using `model.evaluate()` on `train_final` and `label_train_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 532us/step - loss: 0.3506 - accuracy: 0.8829\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use this function to get the results on our testing set.  Call the function again, but this time on `test` and `label_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 552us/step - loss: 0.6327 - accuracy: 0.7667\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the contents of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3506323993206024, 0.8829333186149597]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.33576024494171142, 0.89600000000000002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.632739245891571, 0.7666666507720947]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.72006658554077152, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results. Let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy.\n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5zklEQVR4nO3deXxU5fX48c/JZGFJ2MJOwAAiOwQMQtiMQCuIdaFYpQgi1K1aa239Ku1PobXW9qu26LdWi2tRFBfcpS4gARQUgoR9hwQCyBKWgEBCkvP7Y+7ESUgmk2UySea8+8qLmXuf+9xzEztnnuU+V1QVY4wxoSss2AEYY4wJLksExhgT4iwRGGNMiLNEYIwxIc4SgTHGhDhLBMYYE+IsEZhqIyL/FZGbqrpsTSYiU0TkS6/3p0Skkz9lK3CugPzORORlEflzVddrao7wYAdgajYROeX1tgGQA+Q7729T1bn+1qWqYwJRtrxEpBnwH2A48D0wS1X/N1Dn86aq0VVRj4jMBC5U1Ru96g7Y78zUbZYIjE/eH1wikg78QlUXFi8nIuGqmledsVXCfUA9oA0QBfQIbjjGBJd1DZkKEZFkEckUkftF5DvgJRFpKiIfichhETnmvI7zOiZFRH7hvJ4iIl+KyONO2d0iMqaCZTuKyFIROSkiC0XkaRF51Uf4ecAhVT2tqsdU9asyrvVZEXm82Lb3ReRe5/UDIrLTOf8mEbnWR10qIhc6r2NF5AMRyRaRlUDnYmWfFJG9zv7VIjLM2T4a+D1wvdPVtLaE31mYiPw/EckQkUMiMkdEGjv74p04bhKRPSJyRET+4Ot3UCyuW0Rkh4gcdeJv62wXEfmHc74TIrJORHo5+65wfjcnRWSfiPzO3/OZwLNEYCqjNdAMuAC4Ffd/Ty857zsAZ4B/+jh+ILAVaA78L/CCiEgFyr4GrARigZnApDLiXglMEJGpZZTzeA33h64AiEhT4MfAPGf/TmAY0Bj4I/CqiLTxo96ngbO4WyZTnR9vq4AE3L/j14C3RKSeqn4C/AV4Q1WjVbVvCXVPcX4uAzoB0Zz/txgKdAVGAg+JSPeyAhaREcCjwM+cuDP44ffwY9zdbRcBTYDrgSxn3wu4uxJjgF7AF2Wdy1QfSwSmMgqAGaqao6pnVDVLVec737RPAo8Al/o4PkNVn1PVfNx99m2AVuUpKyIdgAHAQ6qaq6pfAh+UdkLn2/hsIBl4QERudrZHiUiu51tzMcsAxf1hDzAeWKGq+wFU9S1V3a+qBar6BrAduMTHdSMiLuCnTtzfq+oG57oKqeqrzu80T1WfwN2N1dVXvV4mAn9X1V2qegqYDtwgIt7dwX90/m5rgbVASQmlpHpfVNVvVTXHqTdJROKBc0AM0A0QVd2sqgec484BPUSkkdMK+9bP6zDVwBKBqYzDqnrW80ZEGojIv53uiGxgKdDE+dAryXeeF6p62nlZ2mBqaWXbAke9tgHs9RHzNOBzVV0KXA487CSDQcAaVT1R/AB1r8w4D5jgbPo5UDhILiKTRSRNRI6LyHHc33ib+4gBoAXuMTrvWDO8C4jIb0Vks9PNchx3i6Osej3aFqsvwzmfd6L9zuv1aUr/3Zdar5NksoB2qvoF7lbH08BBEZktIo2coj8FrgAyRGSJiCT5eR2mGlgiMJVRfOna3+L+xjpQVRvh7iYAKK27pyocAJqJSAOvbe19lA/HPUaAqu4GRuPuanoe+JOP414HxovIBbi7qeYDOO+fA+4CYlW1CbCBsq/5sBOHd6wdPC+c8YD7cXfBNHXqPeFVb1nLBu/H3UXnXXcecLCM48pSpF4RaYi7S24fgKo+paoXAz1xdxHd52xfpapXAy2B94A3KxmHqUKWCExVisE9LnBc3FM0ZwT6hKqaAaQCM0Uk0vmm+RMfh7yDu7//Gqelko27W6QzPj5cVXUN7g/v54FPVfW4s6uhc9xhAKd10cuPuPOdWGY6LakegPc9ADG4P7gPA+Ei8hDQyGv/QSBeREr7//DrwG+cgfRofhhTqOzMrteAm0UkQUSinHq/UdV0ERkgIgNFJAL3tNyzQL7zd5koIo1V9Rzu33l+6acw1c0SgalKs4D6wBHga+CTajrvRCAJdxfFn4E3cN/vcB5VXYG7a2cGcAz4FFiAu+vidRHp5+M8rwOjcH8YeurbBDwBrMD94dwb8DkLyctduLtjvgNexj3Q7vEp8F9gG+6umLMU7UZ6y/k3S0RK6m9/EXgFd/fcbuf4X/kZV6lUdRHwIO4W0QHcCfQGZ3cj3K2jY07MWYBnttUkIN3pMrwduBFTY4g9mMbUNSLyBrBFVQPeIjGmLrAWgan1nC6Jzs7c+dHA1bj7oY0xfrA7i01d0Bp3f3sskAnc4fTpG2P8YF1DxhgT4qxryBhjQlyt6xpq3ry5xsfHBzsMY4ypVVavXn1EVVuUtC9giUBE2gNzcPffFgCzVfXJYmUEeBL3HYengSll3XoeHx9PampqYII2xpg6SkQyStsXyBZBHvBbVf1WRGKA1SLyuTPv2mMM0MX5GQg84/xrjDGmmgRsjEBVD3i+3TsLkG0G2hUrdjUwR92+xr0ujT+rNhpjjKki1TJY7KxM2A/4ptiudhS9WzKT85OFMcaYAAr4YLGzzsl84B5VzS6+u4RDzpvPKiK34l7vng4dOpx3gDEmsM6dO0dmZiZnz54tu7AJqnr16hEXF0dERITfxwQ0ETiLT80H5qrqOyUUyaTo6otxuFc3LEJVZ+NeQ57ExES78cGYapaZmUlMTAzx8fGU/uwgE2yqSlZWFpmZmXTs2NHv4wLWNeTMCHoB2Kyqfy+l2AfAZOcRd4OAE14PsjDG1BBnz54lNjbWkkANJyLExsaWu+UWyBbBENwrDq4XkTRn2+9x1lxX1Wdxr/p4BbAD9/TRmwMYjzGmEiwJ1A4V+TsFLBE4jwz0GZHz5Kc7AxWDt4zjGfzj63/w2I8eI8Llf9+ZMcbUdSGzxETad2k8+c2TzPp6VrBDMcaUU1ZWFgkJCSQkJNC6dWvatWtX+D43N9fnsampqdx9991lnmPw4MFVEmtKSgpXXnllldRVXWrdEhMV1bJhS7o3786Dix/k+l7X06GxzT4ypraIjY0lLS0NgJkzZxIdHc3vfve7wv15eXmEh5f8cZaYmEhiYmKZ51i+fHmVxFobhUSLYMXeFYycM5KtWVvJyc/hxnfs4UjG1HZTpkzh3nvv5bLLLuP+++9n5cqVDB48mH79+jF48GC2bt0KFP2GPnPmTKZOnUpycjKdOnXiqaeeKqwvOjq6sHxycjLjx4+nW7duTJw4Ec8qzQsWLKBbt24MHTqUu+++u8xv/kePHuWaa66hT58+DBo0iHXr1gGwZMmSwhZNv379OHnyJAcOHGD48OEkJCTQq1cvli1bVuW/s9KERIsgJT2F3PxcCrQAQVi2ZxkfbP2Aq7peFezQjKl17vnkHtK+S6vSOhNaJzBr9KxyH7dt2zYWLlyIy+UiOzubpUuXEh4ezsKFC/n973/P/Pnzzztmy5YtLF68mJMnT9K1a1fuuOOO8+bcr1mzho0bN9K2bVuGDBnCV199RWJiIrfddhtLly6lY8eOTJgwocz4ZsyYQb9+/Xjvvff44osvmDx5MmlpaTz++OM8/fTTDBkyhFOnTlGvXj1mz57N5Zdfzh/+8Afy8/M5ffp0uX8fFRUSiSA5PplIVyS5+blEuiJpHd2aWz+8lcS2ibSNaRvs8IwxFXTdddfhcrkAOHHiBDfddBPbt29HRDh37lyJx4wdO5aoqCiioqJo2bIlBw8eJC4urkiZSy65pHBbQkIC6enpREdH06lTp8L5+RMmTGD27Nk+4/vyyy8Lk9GIESPIysrixIkTDBkyhHvvvZeJEycybtw44uLiGDBgAFOnTuXcuXNcc801JCQkVOZXUy4hkQiS2iexaPIiUtJTSI5PZtexXdz8/s2MfnU0qbemEumKDHaIxtQaFfnmHigNGzYsfP3ggw9y2WWX8e6775Kenk5ycnKJx0RFRRW+drlc5OXl+VWmIg/xKukYEeGBBx5g7NixLFiwgEGDBrFw4UKGDx/O0qVL+fjjj5k0aRL33XcfkydPLvc5KyIkxgjAnQymD5sOwC0f3kJ+QT7rD61n4vyJQY7MGFMVTpw4Qbt27qXKXn755Sqvv1u3buzatYv09HQA3njjjTKPGT58OHPnzgXcYw/NmzenUaNG7Ny5k969e3P//feTmJjIli1byMjIoGXLltxyyy1MmzaNb7/1uSJ/lQqZROBROF6Ae7zg7c1v89r614IdljGmkv7nf/6H6dOnM2TIEPLz86u8/vr16/Ovf/2L0aNHM3ToUFq1akXjxo19HjNz5kxSU1Pp06cPDzzwAP/5z38AmDVrFr169aJv377Ur1+fMWPGkJKSUjh4PH/+fH79619X+TWUptY9szgxMVEr82Aazwwiz3hBu5h2ZJzI4KWrX2JiH2sdGFOSzZs3071792CHEXSnTp0iOjoaVeXOO++kS5cu/OY3vwl2WOcp6e8lIqtVtcR5tCHXIvCMFzx82cPMGj2LzJOZnCs4x6R3J7Fw18Jgh2eMqcGee+45EhIS6NmzJydOnOC2224LdkhVIuQSAfwwXpB1Ootz+e6ZBYrym09/U6EBIWNMaPjNb35DWloamzZtYu7cuTRo0CDYIVWJkEwEHp5ppS5xEREWwYZDG3hxzYvBDssYY6pVSEwfLY33tNJm9Zvxv8v/lzsX3MnQDkPp2rxrsMMzxphqEdKJANzJACgcQM7XfK6adxXr71hv9xcYY0JCSHcNeXimlOZrPmESxrasbfxpyZ+CHZYxxlQLSwQUHSuIckUxqN0gHv3yUV5Z+0qwQzPGAMnJyXz66adFts2aNYtf/vKXPo/xTDW/4oorOH78+HllZs6cyeOPP+7z3O+99x6bNm0qfP/QQw+xcGHlZxjWpOWqA/moyhdF5JCIbChlf2MR+VBE1orIRhEJ2tPJik8pTTuYRoEWcNN7N7FsT/WtAGiMKdmECROYN29ekW3z5s3za+E3cK8a2qRJkwqdu3gi+NOf/sSoUaMqVFdNFcgWwcvAaB/77wQ2qWpfIBl4QkSC1ilf2pTSR5c9GqyQjKnVVuxdwaPLHmXF3hWVrmv8+PF89NFH5OTkAJCens7+/fsZOnQod9xxB4mJifTs2ZMZM2aUeHx8fDxHjhwB4JFHHqFr166MGjWqcKlqcN8jMGDAAPr27ctPf/pTTp8+zfLly/nggw+47777SEhIYOfOnUyZMoW3334bgEWLFtGvXz969+7N1KlTC+OLj49nxowZ9O/fn969e7Nlyxaf1xfs5aoDlghUdSlw1FcRIMZ5yH20U/b81Z+qmXc3UZiEsWj3IjKzM4MdljG1iucO/gcXP8jIOSMrnQxiY2O55JJL+OSTTwB3a+D6669HRHjkkUdITU1l3bp1LFmypPBDtCSrV69m3rx5rFmzhnfeeYdVq1YV7hs3bhyrVq1i7dq1dO/enRdeeIHBgwdz1VVX8dhjj5GWlkbnzp0Ly589e5YpU6bwxhtvsH79evLy8njmmWcK9zdv3pxvv/2WO+64o8zuJ89y1evWreMvf/lL4WJznuWq09LSWLZsGfXr1+e1117j8ssvJy0tjbVr11bJKqXBHCP4J9Ad2A+sB36tqgUlFRSRW0UkVURSDx8+HNCgvLuJ3rruLQTh94t+H9BzGlPXeE/AyM3PJSU9pdJ1encPeXcLvfnmm/Tv359+/fqxcePGIt04xS1btoxrr72WBg0a0KhRI6666odnkmzYsIFhw4bRu3dv5s6dy8aNG33Gs3XrVjp27MhFF10EwE033cTSpUsL948bNw6Aiy++uHChutJ8+eWXTJo0CSh5ueqnnnqK48ePEx4ezoABA3jppZeYOXMm69evJyYmxmfd/ghmIrgcSAPaAgnAP0WkUUkFVXW2qiaqamKLFi0CHpinm6hNdBuS4pJ4Zd0rrNy3MuDnNaau8G5ZR7oiSY5PrnSd11xzDYsWLeLbb7/lzJkz9O/fn927d/P444+zaNEi1q1bx9ixYzl79qzPetydEOebMmUK//znP1m/fj0zZswos56yViHwLGVd2lLXZdXlWa76+eef58yZMwwaNIgtW7YULlfdrl07Jk2axJw5c3zW7Y9gJoKbgXfUbQewG+gWxHiK8DRtPYPF096fZstPGOMn75b1osmLCu/XqYzo6GiSk5OZOnVqYWsgOzubhg0b0rhxYw4ePMh///tfn3UMHz6cd999lzNnznDy5Ek+/PDDwn0nT56kTZs2nDt3rnDpaICYmBhOnjx5Xl3dunUjPT2dHTt2APDKK69w6aWXVujagr1cdTBvKNsDjASWiUgroCuwK4jxFFHk3gLC2HB4A29sfIMbet0Q7NCMqRWS2idVSQLwNmHCBMaNG1fYRdS3b1/69etHz5496dSpE0OGDPF5fP/+/bn++utJSEjgggsuYNiwYYX7Hn74YQYOHMgFF1xA7969Cz/8b7jhBm655RaeeuqpwkFigHr16vHSSy9x3XXXkZeXx4ABA7j99tsrdF0zZ87k5ptvpk+fPjRo0KDIctWLFy/G5XLRo0cPxowZw7x583jssceIiIggOjq6SloEAVuGWkRexz0bqDlwEJgBRACo6rMi0hb3zKI2gAB/VdVXy6q3sstQ+6v4ctVtY9oSJmFsunMT4WEhf0O2CTG2DHXtUt5lqAP2iaaqPif4qup+4MeBOn9lFX+85ZKMJUxfNJ0Zi2fwyMhHgh2eMcZUGbuz2Afvx1t6lpx49MtHWZqx1NdhxhhTq1gi8INnvADcN5n9Y8U/ghyRMdXPJkvUDhX5O1ki8EPxm8y+2vsVOXk5wQ7LmGpTr149srKyLBnUcKpKVlYW9erVK9dxNurpB+/xggYRDbjn03uYs3YOt1x8S7BDM6ZaxMXFkZmZSaBv6DSVV69ePeLi4sp1TMg9vL6ylu9Zzs/e/hkA6fek2wwiY0ytYA+vryIr9q5g1CujOHDyAPtO7uORpTZ7yBhT+1kiKAfPoHEB7iWRnkl9xvpMjTG1niWCciiyfkpYJAe/P8jH2z8OdljGGFMplgjKwXv9lIWTFxLfJJ6/fvnXYIdljDGVYiOd5eS9fso9A+/hnk/v4ZvMbxgYNzDIkRljTMVYi6ASerbsSZQryp5XYIyp1SwRVNCKvSu46vWryM3P5Yv0L5i/aX6wQzLGmAqxRFBBnhlEinvW0N9X/D3IERljTMVYIqgg7xlELnHx7YFvOXbmWLDDMsaYcrNEUEHeM4heuOoFzuaf5YU1LwQ7LGOMKTdLBJXgWab6otiL6NikI48vf5y8At/PJjXGmJomYIlARF4UkUMissFHmWQRSRORjSKyJFCxBJLnSWYZxzM4+P1B/vbl34IdkjHGlEsgWwQvA6NL2ykiTYB/AVepak/gugDGEjDFl52YvXp2kCMyxpjyCVgiUNWlwFEfRX4OvKOqe5zyhwIVSyB5DxpHhEWwJ3sP32R+E+ywjDHGb8EcI7gIaCoiKSKyWkQml1ZQRG4VkVQRSa1p66F7DxovmLiARlGNmPXNrGCHZYwxfgtmIggHLgbGApcDD4rIRSUVVNXZqpqoqoktWrSozhj94hk0HtVpFNP6TePtTW+zL3tfsMMyxhi/BDMRZAKfqOr3qnoEWAr0DWI8VWJw+8HkFeTZshPGmFojmIngfWCYiISLSANgILA5iPFU2oq9K5j8rruHa866OaTsTgluQMYY44dATh99HVgBdBWRTBGZJiK3i8jtAKq6GfgEWAesBJ5X1VKnmtYGnhlEHv+36v+CGI0xxvgnYMtQq+oEP8o8BjwWqBiqm2cGUW5+LgVawNrv1qKqiEiwQzPGmFLZ8wiqkGcGUUp6Ctk52fz1q7+yJGMJyfHJwQ7NGGNKJbXtmbuJiYmampoa7DDKdObcGVo/3pq4xnE8/5PnCx9mY4wxwSAiq1U1saR9ttZQgKR9l8bpvNNsOryJEXNGsGLvimCHZIwxJbJEECAp6Sl4Wls5eTmkpKcENyBjjCmFJYIA8QwcAyjKwHb2TGNjTM1kiSBAPAPHt158KwA7ju0IckTGGFMySwQBlNQ+iWfHPkv/Nv158psnqW0D88aY0GCJIMBEhLFdxrLp8Cae/ObJYIdjjDHnsUQQYCv2ruDx5Y8D8NvPfmuzh4wxNY4lggDzXnaiQAt4c+ObQY7IGGOKskQQYN4PrgFIP54e3ICMMaYYW2IiwLyXnVieuZxPd35K1uksYhvEBjs0Y4wBrEVQLTwPrvnLiL9wJu8MP5//cxsrMMbUGJYIqtGp3FOESRif7fqMkXNGWjIwxtQIlgiqUUp6Cji3EtiyE8aYmsISQTVKjk8mKjwKcC87cekFlwY5ImOMCewTyl4UkUMi4vOpYyIyQETyRWR8oGKpKTwDx9d2uxZFycnPCXZIxhgT0BbBy8BoXwVExAX8Dfg0gHHUKEntk5g7bi4tGrTgiRVPBDscY4wJXCJQ1aXA0TKK/QqYDxwKVBw1Uf2I+vzkop/w8faPeX3968EOxxgT4oI2RiAi7YBrgWf9KHuriKSKSOrhw4cDH1yArdi7gtc2vAbApHcn2ewhY0xQBXOweBZwv6rml1VQVWeraqKqJrZo0SLwkQVYSnoK5/LPAZCv+Xy47cMgR2SMCWXBvLM4EZgnIgDNgStEJE9V3wtiTNXCs+xEbn4u+ZpPZnZmsEMyxoSwoCUCVe3oeS0iLwMfhUISgKLLTny28zM+3PYhJ3NOEhMVE+zQjDEhKJDTR18HVgBdRSRTRKaJyO0icnugzlmbeJad+Ouov3L87HFuePsGGyswxgRFwFoEqjqhHGWnBCqOmq5ACwiTMBbsWMDi9MUsmryIpPZJwQ7LGBNC7M7iILNlJ4wxwWaJIMiKLzsx7IJhQY7IGBNqLBEEmWfg+IZeN6Ao3536LtghGWNCjCWCGiCpfRKvXvsqcTFx/PqTX7N8z/Jgh2SMCSGWCGqIlftWcvD7g+w/uZ8Rc0bYDCJjTLWxRFBDpKSnUKAFAOTk26CxMab6WCKoITx3G4c5f5Km9ZsGOSJjTKiwRFBDeAaNH0p+iCZRTXh/6/vBDskYEyKCudaQKSapfRJJ7ZOIDIvk91/8nrsW3MXE3hPtBjNjTEBZi6AGSmybCMDTq562h9wbYwLOEkENlLo/FUEAyM3PtYFjY0xAWSKogZLjk6kXXg9w322cHJ8c3ICMMXWaJYIayDNwPKrTKAq0gLnr51r3kDEmYCwR1FBJ7ZO4f8j9gI0VGGMCyxJBDbZq3yobKzDGBFwgH0zzoogcEpENpeyfKCLrnJ/lItI3ULHUVjZWYIypDn4lAhFpKCJhzuuLROQqEYko47CXgdE+9u8GLlXVPsDDwGx/YgklnrGCMReOoUALyNf8YIdkjKmDRFXLLiSyGhgGNAW+BlKB06o6sYzj4nE/i7hXGeWaAhtUtV1ZsSQmJmpqamqZMdclp8+dpvNTnWkd3ZrrelzHZfGX2U1mxphyEZHVqppY0j5/u4ZEVU8D44D/U9VrgR5VFSAwDfhvqScXuVVEUkUk9fDhw1V42tqhQUQDbuh5A2nfpfHgFw/awLExpkr5nQhEJAmYCHzsbKuS5SlE5DLcieD+0sqo6mxVTVTVxBYtWlTFaWudZvWbAVBAgQ0cG2OqlL+J4B5gOvCuqm4UkU7A4sqeXET6AM8DV6tqVmXrq8tGdRpFRJh7WMYV5rKBY2NMlfErEajqElW9SlX/5gwaH1HVuytzYhHpALwDTFLVbZWpKxQktU9i0U2LaN6gOW1j2nJJu0uCHZIxpo7wd9bQayLSSEQaApuArSJyXxnHvA6sALqKSKaITBOR20XkdqfIQ0As8C8RSROR0BoBroBhHYbxzNhnSD+ezsyUmTy67FEbKzDGVJq/s4bSVDVBRCYCF+Puz1/tTP2sVqE4a8ibqtLjXz3YemQrYRJGpCuSRZMX2SwiY4xPVTFrKMK5b+Aa4H1VPQeUnUFMlRMRBscNRlHyNd8Gjo0xleZvIvg3kA40BJaKyAVAdqCCMr79ov8vCHPf30ekK9IGjo0xleLvYPFTqtpOVa9QtwzgsgDHZkqR1D6J/1zzHwQhKS6JlPQUGyswxlSYX/cCiEhjYAYw3Nm0BPgTcCJAcZky3NjnRt7c+CYfbvuQlIwUolxRNlZgjKkQf7uGXgROAj9zfrKBlwIVlPFPr5bulTsK1G4yM8ZUnL+JoLOqzlDVXc7PH4FOgQzMlO0nF/2E8DB3oy48LNzGCowxFeJvIjgjIkM9b0RkCHAmMCEZfyW1T+LzGz+nSVQT2jVqR57m2b0Fxphy8/c+gr7AHKCxs+kYcJOqrgtgbCUK9fsISjJ/03zGvzWeiLAICrTA7i0wxpyn0vcRqOpaVe0L9AH6qGo/YEQVxmgqYVz3cXRq2olzBefs3gJjTLmV6wllqpqtqp77B+4NQDymAkSEPyb/0f0asXsLjDHlUplHVUqVRWEq7cY+N/KzHj9DUe4ZdI/dW2CM8ZtfYwQlHiiyR1U7VHE8ZbIxgtKdOHuCTk914vjZ44UtAxsrMMZAJcYIROSkiGSX8HMSaBuQaE2FNa7XmJEdRxY+39jGCowx/vCZCFQ1RlUblfATo6pV8oQyU7XuGXhP4TpEEa4IYhvE2pRSY4xP9mFexwzuMJhXrn2FSe9OolfLXtzzyT3k5udaN5ExplSVGSw2NdTPe/+cPyb/kdT9qeTk5Vg3kTHGp4AlAhF5UUQOiciGUvaLiDwlIjtEZJ2I9A9ULKHo/iH3c2HTCymgAJe4cIW52HNij3URGWPOE8gWwcvAaB/7xwBdnJ9bgWcCGEvIiXBF8MZ1bxBGGJ2bdUYQnvv2OUbOGWnJwBhTRMASgaouBY76KHI1MMd5vsHXQBMRaROoeEJR/zb9mT5sOtuytnEu3+46NsaULJhjBO2AvV7vM51t5xGRW0UkVURSDx8+XC3B1RUPXfoQnZt2LuwiinRF2kwiY0wRwZw1VNKdySXe3aaqs4HZ4L6hLJBB1TWRrkje/tnbJM5OpGfLntw54E6bSWSMKSKYLYJMoL3X+zhgf5BiqdMSWicwM3km6w6uY9GuReTm51o3kTGmUDATwQfAZGf20CDghKoeCGI8ddoDQx9gaIehfLjtQyJcETaTyBhTKJDTR18HVgBdRSRTRKaJyO0icrtTZAGwC9gBPAf8MlCxGPcTzOaOm0u98HrEN4lnar+pNpPIGAMEcIxAVSeUsV+BOwN1fnO+Do078MJVLzDuzXHE1o8lryCvSBeRjRUYE5rszuIQc233a7lrwF18tfcrXGEum0lkjLG1hkLRE5c/weoDq1lzYA2/vOSXdG/R3WYSGRPCrEUQgiJdkbx13Vs0qteIBTsWsC97X5GZRHPWzrHWgTEhxFoEIapdo3a8dd1bjJwzkoW7FhLpiiQ3PxdXmIuX0l4iryDPWgfGhAhrEYSw4RcM58nRT7I8cznje4zn4cseZmrC1PMGkY0xdZslghB3R+Id3Hbxbbyy7hXim8Qzue9kIl2Rdp+BMSHEEkGIExGeGvMUwy8Yzs3v38y5gnMsmryIW/rfYvcZGBMiLBEYIl2RvHv9u3Rs2pGrXr+KmKgYOjTuUKSLyAaQjam7LBEYAJrVb8YnEz+hYWRDxswdQ7fm3Yp0Eb2U9hIPLn6Q5P8kc8dHd1hCMKYOEfcNvrVHYmKipqamBjuMOmvdwXUMf2k4raJb8fcf/511B9ex58Qenvv2OfI1HwBBqBdez2YUGVOLiMhqVU0saZ+1CEwRfVr1YcHEBWRmZzJ90XRuS7ytcABZnJXDFbXuImPqEGsRmBIt3LWQsa+NpU+rPnw+6XM2H97MnLVzCu8xcIW5EMTuNzCmlrAWgSm3UZ1GMf9n81n73Vp+9MqP6Na8G89c+QyLb1p83v0GOXk5zEyZaS0DY2opaxEYnz7a9hE/ffOn9GrZi88nfU6z+s0AWLF3BSPnjCQnL4cCCgiTMKJcUdYyMKaGshaBqbArL7qSd69/l42HNjL8peHsy94HQFL7JBZNXsSoTqMIkzAKtMDGDYyppaxFYPyyePdirp53NU3rN+XTGz+lW/NuwA8tA886RZ5xA1eYi6kJU5ncd7K1EIypAYLWIhCR0SKyVUR2iMgDJexvLCIfishaEdkoIjcHMh5TcZd1vIwlU5aQk5fDkBeHsHzvcuCHlkFJ6xT9e/W/7a5kY2qBQD6q0gU8DYwBegATRKRHsWJ3AptUtS+QDDwhIpGBislUTr82/fhq6lfE1o9l5JyRvLP5HcCdDKYPm27TTI2ppQLZIrgE2KGqu1Q1F5gHXF2sjAIxIiJANHAUyAtgTKaSOjfrzPJpy+nXuh/j3xzPE8ufwNO96Gkd3HbxbUS5os67K3nknJHMXj3bkoIxNUzAxghEZDwwWlV/4byfBAxU1bu8ysQAHwDdgBjgelX9uIS6bgVuBejQocPFGRkZAYnZ+O/MuTNMencS8zfPZ0rCFJ4d+yxR4VGF+1fsXUFKekqRu5LDCMMV5qJAC2wMwZhq5muMIJCJ4Drg8mKJ4BJV/ZVXmfHAEOBeoDPwOdBXVbNLq9cGi2uOAi3gT0v+xB+X/JGkuCTeuu4t2jVqV6SM92CyiFCgBRRoAfDDUhWzRs8i63QWyfHJlhSMCRBfiSCQTyjLBNp7vY8D9hcrczPwV3Vnox0isht362BlAOMyVSRMwpiZPJOeLXpy8/s30+/f/Zg7bi4/6vyjwjKe7qKU9BRiG8Ryzyf3cDbvLOr8Lycvh7sW3GWtBGOCKJAtgnBgGzAS2AesAn6uqhu9yjwDHFTVmSLSCvgWd4vgSGn1WougZtpyZAvj3xzPpsOb+MOwPzAjeQbhYed/z1ixd0WRpSqslWBM9QhK15Bz4iuAWYALeFFVHxGR2wFU9VkRaQu8DLQBBHfr4FVfdVoiqLm+z/2eX/33V7yU9hJJcUnMHTeXjk07lljWM4ZQvJUA2FiCMQEQtEQQCJYIar55G+Zx20e3AfDEj59gWr9puCeGlawirQSAlPQUazEY4ydLBKbapR9PZ8p7U1iSsYRRnUYx+8rZpbYOPMrTSrA7mI0pH0sEJigKtIDZq2fzP5//D/maz58v+zN3D7wbV5irzGN9tRK8b1iD81sMsQ1ibXzBmGIsEZig2ntiL3d8fAcfb/+YAW0H8MzYZ7i47cV+HVu8leC9plFufu55LYb8gvwiq6FacjDGzRKBCTpV5c2Nb3L3J3dz+PvDTO03lUdGPEKr6FZ+1+FJCp4xAl/jCmDJwRhvlghMjXHi7AkeXvowT37zJPXD6/Pg8Ae5e+DdRe5KLo/iLQbv5yN4lscuT3KwgWhTV1kiMDXO1iNb+d3nv+OjbR/RqWknHh35KON7jCdMKr78lXdS8HzjL09y8DUQDRSp25KEqW0sEZga67Odn/Hbz37LhkMb6NuqL38e8WfGdhnrc7ppeZQ3OZQ0EB3hikAQzuWfK7OLCaw1YWomSwSmRssvyGfehnnMSJnBzmM7GdhuIA9f9jCjOo2qsoTgrbTkUNpAdPHkACV3MYWHhftsTViiMMFkicDUCufyz/Fy2ss8vPRh9mbvZUj7ITw4/EF+3PnHAUkI3nwNRHuSg3eLoKQuJl+tCe96/Ol28rXNkoipCEsEplbJycvh+W+f529f/Y292XsZ0HYAv036LeO6jyPCFVFtcRRPDmV1MXlaBKW1JvztdvLUU9I2f1obZSUUSyKhyRKBqZU8Tzf721d/Y8fRHcQ1iuPOAXdyS/9biG0QG+zwzuti8tWa8H5dVreTP9tKa22UlVDKGtsorQVS1n5PcvFOnpZwahZLBKZWK9ACFmxfwD++/gdf7P6C+uH1ubHPjfzqkl/Ru1XvYIdXopJaE/52O/lqEfjb2ii+38PX2IavFog/LZR+bfoVGWvxp+urPC0Zf+spKTH5m8DKE2NJia6kRFhS3cFIkpYITJ2x/uB6nvrmKV5d/ypn884youMI7hxwJ1dedCWRrtrzuGtf3U6+tpXV2igrofgztlF8m78tFM8UXO/FAn11fVUkEZZVT0mJqaTuu/KW86dlVXzSQWl1l3bfSqDHiCwRmDon63QWz3/7PE+vepq92XuJrR/LhF4TmNZ/GgmtE4IdXkD5am2U9YHia2yjIh/WJS3zkVeQ53fXV0W7xsqqp6TEVFI9/pTzt2VVPMmWVndJ962UJ+lFuiJZNHlRuZOBJQJTZ+UV5LFw10JeTnuZ97a8R05+DoltE5nWbxrje4yneYPmwQ6xxvHVDVLe7hvvFkqkK5JZo2ex5sAav7q+AtEiKGv9KV8JzNdUYH9aViUlwrISRkWSnktcPHzZw0wfNr1cf/dgPphmNPAk7gfTPK+qfy2hTDLuh9dEAEdU9VJfdVoiMKU5euYor657lee+fY4NhzYQHhbOjzr9iIm9J3Jt92tpENEg2CHWSb76xSs7EF3eZFVSYippYNzfcuWZNebp8vFOhCXVXdJ9K3W2RSAiLtyPqvwR7ucXrwImqOomrzJNgOXAaFXdIyItVfWQr3otEZiyqCprD65l3oZ5vL7hdfac2EN0ZDTje4xnXLdxjOo0ivoR9YMdpgkQf2cuVXaGk69B4LLqrugYUa0bIxCRJGCmql7uvJ8OoKqPepX5JdBWVf+fv/VaIjDlUaAFLMtYxpy1c3h789tk52TTIKIBY7uM5fqe13NFlyssKZiQEKxEMB73N/1fOO8nAQNV9S6vMrNwdwn1BGKAJ1V1Tgl13QrcCtChQ4eLMzIyAhKzqdty83NZkr6Edza/wztb3uHQ94doGNGQK7pcwbju4xhz4Rga12sc7DCNCYhgJYLrgMuLJYJLVPVXXmX+CSQCI4H6wApgrKpuK61eaxGYqpBXkEdKegpvbXyL97a+x6HvDxEeFs7g9oMZc+EYxlw4hj6t+gR8aQtjqouvRFDxNX/Llgm093ofB+wvocwnqvq9qh4BlgJ9AxiTMQCEh4UzqtMo/v2Tf7P/3v0su3kZ9w2+j+ycbKYvmk7CvxNo/4/23PLBLXyw9QNOnzsd7JCNCZhAtgjCcQ8WjwT24R4s/rmqbvQq0x34J3A5EAmsBG5Q1Q2l1WstAhNoB04e4JMdn/DfHf/l052fkp2TTb3wegztMJQR8SO4rONlJLZNJDwsPNihGuO3YE4fvQL31FAX8KKqPiIitwOo6rNOmfuAm4EC3FNMZ/mq0xKBqU65+bkszVjKR9s+4ovdX7D+0HoAoiOjGX7BcEZ2HMmoTqPo1bJXpR6qY0yg2Q1lxlSRw98fJiU9hcXpi/li9xdszdoKQIsGLRjRcQSjOo0iOT6Zzk072/iCqVEsERgTIJnZmSzctZCFuxbyxe4vOHDqAACtGrZiaIehXHrBpVwaf6m1GEzQWSIwphqoKluObGHZnmV8uedLlmYsJeOEe6pz46jGDIwbyKB2g0hqn8SguEE0qdckuAGbkGKJwJggyTiewZKMJXy15yu+3vc1Gw5tKFyfpmeLngztMJQh7YcwpMMQOjbpaN1JJmAsERhTQ5zMOcmq/atYvnd54c+JnBMAtI5uzcB2A+nbqi99W/fl4jYX06FxB0sOpkpYIjCmhsovyGfj4Y2FSWHlvpVsP7q9sNXgSQ6D4gYxKG4QF7e5mJiomCBHbWojSwTG1CKnz51mw6ENrNq3im/2fcPXmV+z/eh2wL0ccbfm3Uhsm0hi20QGtB1A39Z9bWVVUyZLBMbUckfPHOWbzG9YtX8VqftTWblvJQe/Pwi4k8OFzS6kT6s+9Gvdj/5t+pPQOoHW0a2tW8kUskRgTB2jquw/uZ9V+1ex9ru1rDu0jnUH17Hj6I7CMs3qN6NXy170b92fAe0GkNg2kc5NO+MKcwUxchMslgiMCRHZOdmsObCGtQfXsuHQBtYfWk/ad2mczTsLQJQrim7Nu9G/TX8uaXcJiW0T6d68Ow0jGwY5chNolgiMCWF5BXlsPLSRNd+tYeOhjaw/tJ7VB1Zz5PSRwjIdGnegR4se9Gjeg54te9KnVR96tuhpz2qoQ3wlAls1y5g6LjwsnL6t3VNSPVSVjBMZrN6/ms1HNrP5yGY2Hd5ESnpKYeshTMLo0qwLfVr1oXfL3vRs2ZOeLXrSuVlnW3CvjrG/pjEhSESIbxJPfJP4ItvzC/LZeWwn6w+uZ+3BtYWth7c2vVVYJtIVSdfYrvRs2ZPuzbvTNbYr3Vt0p1vzbkS6Iqv5SkxVsK4hY0yZTuWeYsuRLWw8tJGNhzey6fAmNh7eSMbxDBT3Z0hEWATdW7gTQ3yTeDo37Uzf1n3p06qPTW+tAWyMwBgTEKfPnWZ71nY2Hd7EuoPrWHfIPXMp/Xg6ufm5gLuLqVPTTlwUexEXNbuIbs270a15N7o270qrhq1sims1sURgjKlWBVrAnhN7SPsujTUH1rD5yGa2H93OtqxtRZ721jiqMV2bd6VrbFd3ooi9iK6xXekS28VaEVXMEoExpkYo0AL2Ze9j85HNbD2yla1ZW9lyZAvbsraxN3tvkbLtG7WnS2wXusZ2dc9oatGDi2Ivom1MW1vSuwKC+YSy0cCTuJ9Q9ryq/rWUcgOAr4HrVfVtX3VaIjCmbvJ0M23N2srWI1sLWxBbjmwpXJgPoF54PTo26Uinpp3o1LQTnZt2LmxVtG/c3mY0lSIo00dFxAU8DfwI90PqV4nIB6q6qYRyfwM+DVQsxpiar0FEg/OmuYJ7quuBUwfYdHgT27O2s+vYLnYe28nu47tZtmcZ2TnZhWUjwiLcA9XNOtOpiTtRdG3ele7NuxPfJN7uqi5FIFPnJcAOVd0FICLzgKuBTcXK/QqYDwwIYCzGmFpKRGgb05a2MW0Z1WlUkX2qyuHTh9l6ZCvbsrax4+gOdhzbwa5ju1ixd0WRlkREWAQdGncgvkk8HZt0dCcLp1XRsUlHmtVvFrID14FMBO0A706/TGCgdwERaQdcC4zARyIQkVuBWwE6dOhQ5YEaY2onEaFlw5a0bNiSYRcMO2//0TNH2XJkS+E4RMaJDHYf2837W9/n8OnDRco2impU2OV0YbMLubDZhXRp1oWuzbvSJrpNnU4SgUwEJf3Wig9IzALuV9V8X79kVZ0NzAb3GEFVBWiMqdua1W/G4PaDGdx+8Hn7TuacZNexXaQfT2fXsV3sPr6bXcd2sfnIZj7e/nHh9Fdwd1u1i2lH6+jWtG/cngubXkjnZp3p3LQzHZt2pHV061o9gB3IRJAJtPd6HwfsL1YmEZjnJIHmwBUikqeq7wUwLmOMISYqpsQxCXDfYZ2Zncn2o9vZemQrO47uYP+p/Xx36ju+2vMV8zbMK3x4ELgHsD0D1x2bdCy8a7tD4w60b9yeFg1a1OgWRcBmDYlIOLANGAnsA1YBP1fVjaWUfxn4yGYNGWNqutz83MKWxK5ju9h5dCe7jrv/3X18N6dyTxUpXz+8fmFyuKDxBYVjFZ4xiuYNmgc8UQRl1pCq5onIXbhnA7mAF1V1o4jc7ux/NlDnNsaYQIp0RRbeAFecqnLs7DHSj6ez98ReMk5kkHE8g/QT6ew+tpuV+1aSdSaryDExkTF0buZuTbSNaUu7mHZ0bNqRC5tdWC0D2XZDmTHGVLNTuafIOJ7xQ4vi2M7CpTkOnDrA8bPHi5SPckXRNqYtd11yF/cm3Vuhc9oy1MYYU4NER0a7l/Vu2bPE/d/nfv/D/RLHdnPg1AH2n9xP6+jWAYnHEoExxtQwDSMb0rtVb3q36l0t56u9852MMcZUCUsExhgT4iwRGGNMiLNEYIwxIc4SgTHGhDhLBMYYE+IsERhjTIizRGCMMSGu1i0xISKHgYxyHtYcOBKAcILBrqVmsmupuerS9VTmWi5Q1RYl7ah1iaAiRCS1tDU2ahu7lprJrqXmqkvXE6hrsa4hY4wJcZYIjDEmxIVKIpgd7ACqkF1LzWTXUnPVpesJyLWExBiBMcaY0oVKi8AYY0wpLBEYY0yIq9OJQERGi8hWEdkhIg8EO57yEJH2IrJYRDaLyEYR+bWzvZmIfC4i251/mwY7Vn+JiEtE1ojIR8772nwtTUTkbRHZ4vyNkmrr9YjIb5z/xjaIyOsiUq+2XIuIvCgih0Rkg9e2UmMXkenO58FWEbk8OFGXrJRrecz5b2ydiLwrIk289lXZtdTZRCAiLuBpYAzQA5ggIj2CG1W55AG/VdXuwCDgTif+B4BFqtoFWOS8ry1+DWz2el+br+VJ4BNV7Qb0xX1dte56RKQdcDeQqKq9ABdwA7XnWl4GRhfbVmLszv9/bgB6Osf8y/mcqCle5vxr+Rzopap9gG3AdKj6a6mziQC4BNihqrtUNReYB1wd5Jj8pqoHVPVb5/VJ3B807XBfw3+cYv8BrglKgOUkInHAWOB5r8219VoaAcOBFwBUNVdVj1NLrwf3I2vri0g40ADYTy25FlVdChwttrm02K8G5qlqjqruBnbg/pyoEUq6FlX9TFXznLdfA3HO6yq9lrqcCNoBe73eZzrbah0RiQf6Ad8ArVT1ALiTBdAyiKGVxyzgf4ACr2219Vo6AYeBl5yurudFpCG18HpUdR/wOLAHOACcUNXPqIXX4qW02Gv7Z8JU4L/O6yq9lrqcCKSEbbVurqyIRAPzgXtUNTvY8VSEiFwJHFLV1cGOpYqEA/2BZ1S1H/A9NbfrxCen//xqoCPQFmgoIjcGN6qAqbWfCSLyB9zdxXM9m0ooVuFrqcuJIBNo7/U+DneTt9YQkQjcSWCuqr7jbD4oIm2c/W2AQ8GKrxyGAFeJSDruLroRIvIqtfNawP3fVqaqfuO8fxt3YqiN1zMK2K2qh1X1HPAOMJjaeS0epcVeKz8TROQm4Epgov5w41eVXktdTgSrgC4i0lFEInEPrHwQ5Jj8JiKCuw96s6r+3WvXB8BNzuubgPerO7byUtXpqhqnqvG4/w5fqOqN1MJrAVDV74C9ItLV2TQS2ETtvJ49wCARaeD8NzcS93hUbbwWj9Ji/wC4QUSiRKQj0AVYGYT4/CYio4H7gatU9bTXrqq9FlWtsz/AFbhH2ncCfwh2POWMfSjupt46IM35uQKIxT0TYrvzb7Ngx1rO60oGPnJe19prARKAVOfv8x7QtLZeD/BHYAuwAXgFiKot1wK8jnts4xzub8nTfMUO/MH5PNgKjAl2/H5cyw7cYwGez4BnA3EttsSEMcaEuLrcNWSMMcYPlgiMMSbEWSIwxpgQZ4nAGGNCnCUCY4wJcZYIjHGISL6IpHn9VNndwiIS772qpDE1SXiwAzCmBjmjqgnBDsKY6mYtAmPKICLpIvI3EVnp/FzobL9ARBY5a8UvEpEOzvZWztrxa52fwU5VLhF5zln7/zMRqe+Uv1tENjn1zAvSZZoQZonAmB/UL9Y1dL3XvmxVvQT4J+6VVHFez1H3WvFzgaec7U8BS1S1L+41iDY627sAT6tqT+A48FNn+wNAP6ee2wNzacaUzu4sNsYhIqdUNbqE7enACFXd5SwE+J2qxorIEaCNqp5zth9Q1eYichiIU9Ucrzrigc/V/bAUROR+IEJV/ywinwCncC9V8Z6qngrwpRpThLUIjPGPlvK6tDIlyfF6nc8PY3RjcT9N72JgtfOAGGOqjSUCY/xzvde/K5zXy3GvpgowEfjSeb0IuAMKn9PcqLRKRSQMaK+qi3E/uKcJcF6rxJhAsm8exvygvoikeb3/RFU9U0ijROQb3F+eJjjb7gZeFJH7cD+x7GZn+6+B2SIyDfc3/ztwrypZEhfwqog0xv2wkX+o+7GXxlQbGyMwpgzOGEGiqh4JdizGBIJ1DRljTIizFoExxoQ4axEYY0yIs0RgjDEhzhKBMcaEOEsExhgT4iwRGGNMiPv/58CNvYUOJiwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g.', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! \n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation accuracy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-3f4bab085b80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0macc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_val_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_val_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early stopping\n",
    "\n",
    "Now that we know that the model starts to overfit around epoch 60, we can just retrain the model from scratch, but this time only up to 60 epochs! This will help us with our overfitting problem.  This method is called **_Early Stopping_**.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the exact model we did above. \n",
    "* Compile the model with the exact same hyperparameters.\n",
    "* Fit the model with the exact same hyperparameters, with the exception of `epochs`.  This time, set epochs to `60` instead of `120`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9345 - accuracy: 0.1821 - val_loss: 1.9140 - val_accuracy: 0.2070\n",
      "Epoch 2/60\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.9113 - accuracy: 0.1991 - val_loss: 1.8941 - val_accuracy: 0.2260\n",
      "Epoch 3/60\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8893 - accuracy: 0.2187 - val_loss: 1.8731 - val_accuracy: 0.2460\n",
      "Epoch 4/60\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8655 - accuracy: 0.2428 - val_loss: 1.8490 - val_accuracy: 0.2680\n",
      "Epoch 5/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8398 - accuracy: 0.2649 - val_loss: 1.8230 - val_accuracy: 0.2890\n",
      "Epoch 6/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.8112 - accuracy: 0.2917 - val_loss: 1.7932 - val_accuracy: 0.3030\n",
      "Epoch 7/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7792 - accuracy: 0.3131 - val_loss: 1.7599 - val_accuracy: 0.3390\n",
      "Epoch 8/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.7430 - accuracy: 0.3431 - val_loss: 1.7227 - val_accuracy: 0.3560\n",
      "Epoch 9/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7030 - accuracy: 0.3663 - val_loss: 1.6828 - val_accuracy: 0.3870\n",
      "Epoch 10/60\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6597 - accuracy: 0.3988 - val_loss: 1.6399 - val_accuracy: 0.4180\n",
      "Epoch 11/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6131 - accuracy: 0.4303 - val_loss: 1.5929 - val_accuracy: 0.4390\n",
      "Epoch 12/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5640 - accuracy: 0.4664 - val_loss: 1.5462 - val_accuracy: 0.4760\n",
      "Epoch 13/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5133 - accuracy: 0.5008 - val_loss: 1.4966 - val_accuracy: 0.5030\n",
      "Epoch 14/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4611 - accuracy: 0.5329 - val_loss: 1.4461 - val_accuracy: 0.5290\n",
      "Epoch 15/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4092 - accuracy: 0.5652 - val_loss: 1.3963 - val_accuracy: 0.5540\n",
      "Epoch 16/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3565 - accuracy: 0.5943 - val_loss: 1.3503 - val_accuracy: 0.5740\n",
      "Epoch 17/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3049 - accuracy: 0.6135 - val_loss: 1.3001 - val_accuracy: 0.5960\n",
      "Epoch 18/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2544 - accuracy: 0.6348 - val_loss: 1.2528 - val_accuracy: 0.6060\n",
      "Epoch 19/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2053 - accuracy: 0.6500 - val_loss: 1.2085 - val_accuracy: 0.6260\n",
      "Epoch 20/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1593 - accuracy: 0.6633 - val_loss: 1.1670 - val_accuracy: 0.6410\n",
      "Epoch 21/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1156 - accuracy: 0.6765 - val_loss: 1.1251 - val_accuracy: 0.6360\n",
      "Epoch 22/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0742 - accuracy: 0.6860 - val_loss: 1.0914 - val_accuracy: 0.6510\n",
      "Epoch 23/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0358 - accuracy: 0.6973 - val_loss: 1.0560 - val_accuracy: 0.6580\n",
      "Epoch 24/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0006 - accuracy: 0.7023 - val_loss: 1.0235 - val_accuracy: 0.6660\n",
      "Epoch 25/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9672 - accuracy: 0.7131 - val_loss: 0.9965 - val_accuracy: 0.6710\n",
      "Epoch 26/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9372 - accuracy: 0.7187 - val_loss: 0.9681 - val_accuracy: 0.6750\n",
      "Epoch 27/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9089 - accuracy: 0.7296 - val_loss: 0.9452 - val_accuracy: 0.6810\n",
      "Epoch 28/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8832 - accuracy: 0.7348 - val_loss: 0.9235 - val_accuracy: 0.6850\n",
      "Epoch 29/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8593 - accuracy: 0.7389 - val_loss: 0.9011 - val_accuracy: 0.6910\n",
      "Epoch 30/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8368 - accuracy: 0.7451 - val_loss: 0.8839 - val_accuracy: 0.6900\n",
      "Epoch 31/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8159 - accuracy: 0.7480 - val_loss: 0.8664 - val_accuracy: 0.6940\n",
      "Epoch 32/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7970 - accuracy: 0.7516 - val_loss: 0.8514 - val_accuracy: 0.7000\n",
      "Epoch 33/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7789 - accuracy: 0.7577 - val_loss: 0.8373 - val_accuracy: 0.6980\n",
      "Epoch 34/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7623 - accuracy: 0.7608 - val_loss: 0.8239 - val_accuracy: 0.7030\n",
      "Epoch 35/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7467 - accuracy: 0.7659 - val_loss: 0.8148 - val_accuracy: 0.6950\n",
      "Epoch 36/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7323 - accuracy: 0.7675 - val_loss: 0.8003 - val_accuracy: 0.7120\n",
      "Epoch 37/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7182 - accuracy: 0.7725 - val_loss: 0.7943 - val_accuracy: 0.7130\n",
      "Epoch 38/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7052 - accuracy: 0.7725 - val_loss: 0.7831 - val_accuracy: 0.7100\n",
      "Epoch 39/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.7767 - val_loss: 0.7734 - val_accuracy: 0.7120\n",
      "Epoch 40/60\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.7820 - val_loss: 0.7633 - val_accuracy: 0.7190\n",
      "Epoch 41/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6709 - accuracy: 0.7821 - val_loss: 0.7549 - val_accuracy: 0.7150\n",
      "Epoch 42/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6606 - accuracy: 0.7865 - val_loss: 0.7521 - val_accuracy: 0.7170\n",
      "Epoch 43/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6507 - accuracy: 0.7881 - val_loss: 0.7418 - val_accuracy: 0.7190\n",
      "Epoch 44/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6414 - accuracy: 0.7900 - val_loss: 0.7349 - val_accuracy: 0.7240\n",
      "Epoch 45/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6323 - accuracy: 0.7932 - val_loss: 0.7317 - val_accuracy: 0.7340\n",
      "Epoch 46/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6233 - accuracy: 0.7935 - val_loss: 0.7250 - val_accuracy: 0.7230\n",
      "Epoch 47/60\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6152 - accuracy: 0.7967 - val_loss: 0.7193 - val_accuracy: 0.7270\n",
      "Epoch 48/60\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6079 - accuracy: 0.7992 - val_loss: 0.7168 - val_accuracy: 0.7210\n",
      "Epoch 49/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5998 - accuracy: 0.8008 - val_loss: 0.7127 - val_accuracy: 0.7140\n",
      "Epoch 50/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5923 - accuracy: 0.8031 - val_loss: 0.7103 - val_accuracy: 0.7230\n",
      "Epoch 51/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5852 - accuracy: 0.8057 - val_loss: 0.7039 - val_accuracy: 0.7270\n",
      "Epoch 52/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5784 - accuracy: 0.8091 - val_loss: 0.7008 - val_accuracy: 0.7300\n",
      "Epoch 53/60\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5716 - accuracy: 0.8088 - val_loss: 0.6974 - val_accuracy: 0.7190\n",
      "Epoch 54/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5654 - accuracy: 0.8105 - val_loss: 0.6939 - val_accuracy: 0.7270\n",
      "Epoch 55/60\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5589 - accuracy: 0.8128 - val_loss: 0.6900 - val_accuracy: 0.7340\n",
      "Epoch 56/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5530 - accuracy: 0.8125 - val_loss: 0.6865 - val_accuracy: 0.7330\n",
      "Epoch 57/60\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5470 - accuracy: 0.8163 - val_loss: 0.6822 - val_accuracy: 0.7410\n",
      "Epoch 58/60\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5418 - accuracy: 0.8185 - val_loss: 0.6798 - val_accuracy: 0.7360\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5360 - accuracy: 0.8191 - val_loss: 0.6793 - val_accuracy: 0.7350\n",
      "Epoch 60/60\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5305 - accuracy: 0.8207 - val_loss: 0.6752 - val_accuracy: 0.7420\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did before, get our results using `model.evaluate()` on the appropriate variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 540us/step - loss: 0.5264 - accuracy: 0.8243\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 594us/step - loss: 0.6676 - accuracy: 0.7513\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5263599157333374, 0.8242666721343994]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train  # Expected Output: [0.58606486314137773, 0.79826666669845581]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6676282286643982, 0.7513333559036255]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # [0.74768974288304646, 0.71333333365122475]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument `kernel_regulizers.l2` and adding a value for the regularization parameter lambda between parentheses.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did before.\n",
    "* In our two hidden layers (but not our output layer), add in the parameter `kernel_regularizer=regularizers.l2(0.005)` to add L2 regularization to each hidden layer.  \n",
    "* Compile the model with the same hyperparameters as we did before. \n",
    "* Fit the model with the same hyperparameters as we did before, but this time for `120` epochs.\n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L2_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 2.5887 - accuracy: 0.1719 - val_loss: 2.5724 - val_accuracy: 0.2060\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.5586 - accuracy: 0.2124 - val_loss: 2.5438 - val_accuracy: 0.2300\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.5308 - accuracy: 0.2369 - val_loss: 2.5146 - val_accuracy: 0.2540\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.5013 - accuracy: 0.2579 - val_loss: 2.4842 - val_accuracy: 0.2750\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.4689 - accuracy: 0.2853 - val_loss: 2.4496 - val_accuracy: 0.3070\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.4321 - accuracy: 0.3203 - val_loss: 2.4114 - val_accuracy: 0.3480\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.3897 - accuracy: 0.3603 - val_loss: 2.3692 - val_accuracy: 0.3770\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.3427 - accuracy: 0.3849 - val_loss: 2.3220 - val_accuracy: 0.4100\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.2920 - accuracy: 0.4187 - val_loss: 2.2706 - val_accuracy: 0.4430\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.2393 - accuracy: 0.4479 - val_loss: 2.2192 - val_accuracy: 0.4600\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.1854 - accuracy: 0.4727 - val_loss: 2.1678 - val_accuracy: 0.4790\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.1321 - accuracy: 0.4993 - val_loss: 2.1163 - val_accuracy: 0.5000\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.0801 - accuracy: 0.5139 - val_loss: 2.0688 - val_accuracy: 0.5140\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.0291 - accuracy: 0.5291 - val_loss: 2.0205 - val_accuracy: 0.5340\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.9798 - accuracy: 0.5467 - val_loss: 1.9715 - val_accuracy: 0.5490\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.9323 - accuracy: 0.5633 - val_loss: 1.9285 - val_accuracy: 0.5660\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.8871 - accuracy: 0.5815 - val_loss: 1.8876 - val_accuracy: 0.5690\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.8448 - accuracy: 0.5932 - val_loss: 1.8460 - val_accuracy: 0.5980\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.8043 - accuracy: 0.6089 - val_loss: 1.8086 - val_accuracy: 0.6060\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.7663 - accuracy: 0.6245 - val_loss: 1.7740 - val_accuracy: 0.6150\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.7303 - accuracy: 0.6373 - val_loss: 1.7424 - val_accuracy: 0.6200\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6963 - accuracy: 0.6541 - val_loss: 1.7109 - val_accuracy: 0.6340\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6644 - accuracy: 0.6655 - val_loss: 1.6820 - val_accuracy: 0.6510\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6344 - accuracy: 0.6752 - val_loss: 1.6553 - val_accuracy: 0.6620\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6060 - accuracy: 0.6856 - val_loss: 1.6289 - val_accuracy: 0.6670\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5796 - accuracy: 0.6916 - val_loss: 1.6091 - val_accuracy: 0.6780\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.5542 - accuracy: 0.7011 - val_loss: 1.5836 - val_accuracy: 0.6840\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.5310 - accuracy: 0.7077 - val_loss: 1.5653 - val_accuracy: 0.6890\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5087 - accuracy: 0.7125 - val_loss: 1.5462 - val_accuracy: 0.6880\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4872 - accuracy: 0.7180 - val_loss: 1.5288 - val_accuracy: 0.6870\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4679 - accuracy: 0.7209 - val_loss: 1.5094 - val_accuracy: 0.6940\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4488 - accuracy: 0.7285 - val_loss: 1.4936 - val_accuracy: 0.7050\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4312 - accuracy: 0.7319 - val_loss: 1.4767 - val_accuracy: 0.7050\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4142 - accuracy: 0.7351 - val_loss: 1.4641 - val_accuracy: 0.7040\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.3984 - accuracy: 0.7400 - val_loss: 1.4484 - val_accuracy: 0.7120\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3826 - accuracy: 0.7416 - val_loss: 1.4372 - val_accuracy: 0.7100\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3685 - accuracy: 0.7456 - val_loss: 1.4238 - val_accuracy: 0.7210\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3546 - accuracy: 0.7472 - val_loss: 1.4115 - val_accuracy: 0.7220\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3414 - accuracy: 0.7500 - val_loss: 1.4039 - val_accuracy: 0.7140\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3286 - accuracy: 0.7509 - val_loss: 1.3889 - val_accuracy: 0.7290\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3169 - accuracy: 0.7540 - val_loss: 1.3791 - val_accuracy: 0.7280\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3050 - accuracy: 0.7575 - val_loss: 1.3710 - val_accuracy: 0.7250\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.2937 - accuracy: 0.7627 - val_loss: 1.3608 - val_accuracy: 0.7290\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.2828 - accuracy: 0.7633 - val_loss: 1.3530 - val_accuracy: 0.7270\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2725 - accuracy: 0.7663 - val_loss: 1.3428 - val_accuracy: 0.7340\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2622 - accuracy: 0.7692 - val_loss: 1.3335 - val_accuracy: 0.7290\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2524 - accuracy: 0.7701 - val_loss: 1.3307 - val_accuracy: 0.7350\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.2430 - accuracy: 0.7728 - val_loss: 1.3194 - val_accuracy: 0.7340\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.2336 - accuracy: 0.7725 - val_loss: 1.3176 - val_accuracy: 0.7320\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.2249 - accuracy: 0.7753 - val_loss: 1.3060 - val_accuracy: 0.7320\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.2159 - accuracy: 0.7767 - val_loss: 1.3040 - val_accuracy: 0.7360\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.2077 - accuracy: 0.7787 - val_loss: 1.2931 - val_accuracy: 0.7400\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1997 - accuracy: 0.7804 - val_loss: 1.2890 - val_accuracy: 0.7400\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.1912 - accuracy: 0.7835 - val_loss: 1.2808 - val_accuracy: 0.7450\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.1835 - accuracy: 0.7835 - val_loss: 1.2804 - val_accuracy: 0.7390\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1762 - accuracy: 0.7864 - val_loss: 1.2708 - val_accuracy: 0.7470\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1687 - accuracy: 0.7873 - val_loss: 1.2673 - val_accuracy: 0.7400\n",
      "Epoch 58/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1616 - accuracy: 0.7875 - val_loss: 1.2639 - val_accuracy: 0.7400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1547 - accuracy: 0.7889 - val_loss: 1.2558 - val_accuracy: 0.7480\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1476 - accuracy: 0.7925 - val_loss: 1.2504 - val_accuracy: 0.7480\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1407 - accuracy: 0.7923 - val_loss: 1.2506 - val_accuracy: 0.7500\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1344 - accuracy: 0.7947 - val_loss: 1.2447 - val_accuracy: 0.7480\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.1281 - accuracy: 0.7957 - val_loss: 1.2356 - val_accuracy: 0.7500\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 1.1214 - accuracy: 0.7972 - val_loss: 1.2304 - val_accuracy: 0.7490\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1151 - accuracy: 0.7968 - val_loss: 1.2268 - val_accuracy: 0.7510\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1090 - accuracy: 0.7992 - val_loss: 1.2216 - val_accuracy: 0.7510\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1031 - accuracy: 0.7983 - val_loss: 1.2166 - val_accuracy: 0.7530\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0972 - accuracy: 0.8011 - val_loss: 1.2156 - val_accuracy: 0.7500\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0912 - accuracy: 0.7996 - val_loss: 1.2098 - val_accuracy: 0.7550\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0851 - accuracy: 0.8036 - val_loss: 1.2083 - val_accuracy: 0.7540\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0796 - accuracy: 0.8056 - val_loss: 1.2038 - val_accuracy: 0.7530\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0738 - accuracy: 0.8043 - val_loss: 1.2016 - val_accuracy: 0.7570\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0685 - accuracy: 0.8079 - val_loss: 1.1963 - val_accuracy: 0.7540\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0628 - accuracy: 0.8092 - val_loss: 1.1935 - val_accuracy: 0.7580\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0577 - accuracy: 0.8088 - val_loss: 1.1878 - val_accuracy: 0.7590\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0522 - accuracy: 0.8121 - val_loss: 1.1849 - val_accuracy: 0.7520\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0469 - accuracy: 0.8121 - val_loss: 1.1820 - val_accuracy: 0.7570\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0422 - accuracy: 0.8149 - val_loss: 1.1784 - val_accuracy: 0.7620\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0367 - accuracy: 0.8156 - val_loss: 1.1769 - val_accuracy: 0.7540\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0320 - accuracy: 0.8183 - val_loss: 1.1724 - val_accuracy: 0.7580\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0269 - accuracy: 0.8203 - val_loss: 1.1689 - val_accuracy: 0.7630\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0218 - accuracy: 0.8199 - val_loss: 1.1651 - val_accuracy: 0.7590\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0172 - accuracy: 0.8205 - val_loss: 1.1640 - val_accuracy: 0.7610\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0122 - accuracy: 0.8256 - val_loss: 1.1627 - val_accuracy: 0.7610\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.0130 - accuracy: 0.82 - 0s 6ms/step - loss: 1.0080 - accuracy: 0.8233 - val_loss: 1.1576 - val_accuracy: 0.7630\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.0053 - accuracy: 0.82 - 0s 5ms/step - loss: 1.0029 - accuracy: 0.8261 - val_loss: 1.1537 - val_accuracy: 0.7620\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9980 - accuracy: 0.8288 - val_loss: 1.1535 - val_accuracy: 0.7640\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9934 - accuracy: 0.8288 - val_loss: 1.1490 - val_accuracy: 0.7620\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9888 - accuracy: 0.8305 - val_loss: 1.1439 - val_accuracy: 0.7630\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9847 - accuracy: 0.8295 - val_loss: 1.1450 - val_accuracy: 0.7640\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9799 - accuracy: 0.8323 - val_loss: 1.1402 - val_accuracy: 0.7650\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9758 - accuracy: 0.8328 - val_loss: 1.1377 - val_accuracy: 0.7650\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9712 - accuracy: 0.8345 - val_loss: 1.1366 - val_accuracy: 0.7680\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9668 - accuracy: 0.8351 - val_loss: 1.1364 - val_accuracy: 0.7650\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9628 - accuracy: 0.8363 - val_loss: 1.1298 - val_accuracy: 0.7690\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9585 - accuracy: 0.8355 - val_loss: 1.1274 - val_accuracy: 0.7570\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9542 - accuracy: 0.8391 - val_loss: 1.1248 - val_accuracy: 0.7650\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9501 - accuracy: 0.8413 - val_loss: 1.1243 - val_accuracy: 0.7680\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9463 - accuracy: 0.8405 - val_loss: 1.1193 - val_accuracy: 0.7690\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9418 - accuracy: 0.8428 - val_loss: 1.1180 - val_accuracy: 0.7690\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9377 - accuracy: 0.8451 - val_loss: 1.1166 - val_accuracy: 0.7610\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9340 - accuracy: 0.8463 - val_loss: 1.1117 - val_accuracy: 0.7670\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9297 - accuracy: 0.8445 - val_loss: 1.1092 - val_accuracy: 0.7610\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9257 - accuracy: 0.8444 - val_loss: 1.1071 - val_accuracy: 0.7640\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9219 - accuracy: 0.8468 - val_loss: 1.1060 - val_accuracy: 0.7670\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9177 - accuracy: 0.8472 - val_loss: 1.1063 - val_accuracy: 0.7600\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9144 - accuracy: 0.8476 - val_loss: 1.1005 - val_accuracy: 0.7680\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9107 - accuracy: 0.8521 - val_loss: 1.1007 - val_accuracy: 0.7660\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9065 - accuracy: 0.8532 - val_loss: 1.0993 - val_accuracy: 0.7590\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9031 - accuracy: 0.8531 - val_loss: 1.0961 - val_accuracy: 0.7650\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8991 - accuracy: 0.8535 - val_loss: 1.0960 - val_accuracy: 0.7600\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8956 - accuracy: 0.8533 - val_loss: 1.0936 - val_accuracy: 0.7660\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8917 - accuracy: 0.8549 - val_loss: 1.0923 - val_accuracy: 0.7660\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8883 - accuracy: 0.8568 - val_loss: 1.0868 - val_accuracy: 0.7680\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8844 - accuracy: 0.8579 - val_loss: 1.0897 - val_accuracy: 0.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8814 - accuracy: 0.8549 - val_loss: 1.0823 - val_accuracy: 0.7700\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8776 - accuracy: 0.8585 - val_loss: 1.0824 - val_accuracy: 0.7660\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.8739 - accuracy: 0.8596 - val_loss: 1.0854 - val_accuracy: 0.7680\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.8706 - accuracy: 0.8595 - val_loss: 1.0783 - val_accuracy: 0.7610\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8673 - accuracy: 0.8615 - val_loss: 1.0754 - val_accuracy: 0.7590\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how regularization has affected our model results.  \n",
    "\n",
    "Run the cell below to get the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs).\n",
    "\n",
    "Run the cell below to visualize our training and validation accuracy both with and without L2 regularization, so that we can compare them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-3a3d5a567663>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0macc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL2_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL2_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_val_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did above, but this time, set the `kernel_regularizer` to `regularizers.l1(0.005)` inside both hidden layers. \n",
    "* Compile and fit the model exactly as we did for our L2 Regularization experiment (`120` epochs) \n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L1_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16.0252 - accuracy: 0.1895 - val_loss: 15.6269 - val_accuracy: 0.1790\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 15.2663 - accuracy: 0.2249 - val_loss: 14.8808 - val_accuracy: 0.2070\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14.5277 - accuracy: 0.2477 - val_loss: 14.1522 - val_accuracy: 0.2420\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13.8064 - accuracy: 0.2699 - val_loss: 13.4402 - val_accuracy: 0.2660\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 13.1015 - accuracy: 0.2895 - val_loss: 12.7458 - val_accuracy: 0.2830\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 12.4149 - accuracy: 0.3099 - val_loss: 12.0700 - val_accuracy: 0.3040\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 11.7478 - accuracy: 0.3225 - val_loss: 11.4164 - val_accuracy: 0.3210\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 11.1029 - accuracy: 0.3451 - val_loss: 10.7824 - val_accuracy: 0.3340\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 10.4801 - accuracy: 0.3561 - val_loss: 10.1727 - val_accuracy: 0.3640\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 9.8799 - accuracy: 0.3803 - val_loss: 9.5854 - val_accuracy: 0.3690\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 9.3023 - accuracy: 0.3959 - val_loss: 9.0197 - val_accuracy: 0.3900\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 8.7469 - accuracy: 0.4188 - val_loss: 8.4787 - val_accuracy: 0.4000\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 8.2155 - accuracy: 0.4365 - val_loss: 7.9597 - val_accuracy: 0.4240\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 7.7073 - accuracy: 0.4608 - val_loss: 7.4641 - val_accuracy: 0.4480\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 7.2219 - accuracy: 0.4869 - val_loss: 6.9924 - val_accuracy: 0.4760\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 6.7589 - accuracy: 0.5121 - val_loss: 6.5416 - val_accuracy: 0.5010\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 6.3189 - accuracy: 0.5355 - val_loss: 6.1115 - val_accuracy: 0.5390\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 5.9013 - accuracy: 0.5681 - val_loss: 5.7060 - val_accuracy: 0.5430\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 5.5061 - accuracy: 0.5820 - val_loss: 5.3247 - val_accuracy: 0.5780\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 5.1336 - accuracy: 0.5993 - val_loss: 4.9628 - val_accuracy: 0.5860\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 4.7836 - accuracy: 0.6156 - val_loss: 4.6249 - val_accuracy: 0.5970\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 4.4559 - accuracy: 0.6240 - val_loss: 4.3114 - val_accuracy: 0.6260\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 4.1509 - accuracy: 0.6387 - val_loss: 4.0170 - val_accuracy: 0.6260\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 3.8682 - accuracy: 0.6459 - val_loss: 3.7443 - val_accuracy: 0.6360\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 3.6076 - accuracy: 0.6537 - val_loss: 3.4965 - val_accuracy: 0.6500\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 3.3693 - accuracy: 0.6617 - val_loss: 3.2682 - val_accuracy: 0.6520\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 3.1529 - accuracy: 0.6665 - val_loss: 3.0638 - val_accuracy: 0.6560\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.9581 - accuracy: 0.6713 - val_loss: 2.8800 - val_accuracy: 0.6660\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.7847 - accuracy: 0.6728 - val_loss: 2.7156 - val_accuracy: 0.6640\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.6315 - accuracy: 0.6791 - val_loss: 2.5740 - val_accuracy: 0.6740\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.4977 - accuracy: 0.6803 - val_loss: 2.4534 - val_accuracy: 0.6630\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.3844 - accuracy: 0.6821 - val_loss: 2.3457 - val_accuracy: 0.6760\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.2894 - accuracy: 0.6849 - val_loss: 2.2601 - val_accuracy: 0.6770\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.2134 - accuracy: 0.6840 - val_loss: 2.1938 - val_accuracy: 0.6810\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.1541 - accuracy: 0.6848 - val_loss: 2.1444 - val_accuracy: 0.6880\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.1092 - accuracy: 0.6859 - val_loss: 2.1017 - val_accuracy: 0.6880\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.0758 - accuracy: 0.6868 - val_loss: 2.0711 - val_accuracy: 0.6850\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.0485 - accuracy: 0.6876 - val_loss: 2.0459 - val_accuracy: 0.6900\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 2.0246 - accuracy: 0.6877 - val_loss: 2.0225 - val_accuracy: 0.6890\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 2.0030 - accuracy: 0.6919 - val_loss: 2.0056 - val_accuracy: 0.6910\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9829 - accuracy: 0.6929 - val_loss: 1.9836 - val_accuracy: 0.6870\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9633 - accuracy: 0.6935 - val_loss: 1.9665 - val_accuracy: 0.6870\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.9455 - accuracy: 0.6935 - val_loss: 1.9491 - val_accuracy: 0.6850\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.9282 - accuracy: 0.6937 - val_loss: 1.9311 - val_accuracy: 0.6870\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.9116 - accuracy: 0.6957 - val_loss: 1.9172 - val_accuracy: 0.6900\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8960 - accuracy: 0.6952 - val_loss: 1.8986 - val_accuracy: 0.7010\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.8810 - accuracy: 0.6965 - val_loss: 1.8827 - val_accuracy: 0.6950\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8664 - accuracy: 0.6979 - val_loss: 1.8720 - val_accuracy: 0.6920\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.8521 - accuracy: 0.6980 - val_loss: 1.8579 - val_accuracy: 0.6870\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8387 - accuracy: 0.6983 - val_loss: 1.8424 - val_accuracy: 0.6900\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8250 - accuracy: 0.6983 - val_loss: 1.8284 - val_accuracy: 0.6880\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.8122 - accuracy: 0.6991 - val_loss: 1.8211 - val_accuracy: 0.6850\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.8004 - accuracy: 0.7003 - val_loss: 1.8027 - val_accuracy: 0.6930\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7886 - accuracy: 0.7011 - val_loss: 1.7955 - val_accuracy: 0.6900\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7763 - accuracy: 0.7004 - val_loss: 1.7843 - val_accuracy: 0.6900\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7659 - accuracy: 0.7007 - val_loss: 1.7707 - val_accuracy: 0.6910\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7543 - accuracy: 0.7020 - val_loss: 1.7593 - val_accuracy: 0.6900\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 0s - loss: 1.7455 - accuracy: 0.70 - 0s 4ms/step - loss: 1.7438 - accuracy: 0.7024 - val_loss: 1.7485 - val_accuracy: 0.6900\n",
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.7331 - accuracy: 0.7027 - val_loss: 1.7380 - val_accuracy: 0.7050\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7234 - accuracy: 0.7027 - val_loss: 1.7278 - val_accuracy: 0.6970\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.7137 - accuracy: 0.7036 - val_loss: 1.7208 - val_accuracy: 0.6930\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.7038 - accuracy: 0.7052 - val_loss: 1.7093 - val_accuracy: 0.7110\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6948 - accuracy: 0.7036 - val_loss: 1.6996 - val_accuracy: 0.7060\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6851 - accuracy: 0.7039 - val_loss: 1.6913 - val_accuracy: 0.6990\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6761 - accuracy: 0.7060 - val_loss: 1.6867 - val_accuracy: 0.6960\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.6676 - accuracy: 0.7055 - val_loss: 1.6728 - val_accuracy: 0.7090\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6588 - accuracy: 0.7057 - val_loss: 1.6635 - val_accuracy: 0.7120\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6501 - accuracy: 0.7075 - val_loss: 1.6583 - val_accuracy: 0.7110\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6418 - accuracy: 0.7055 - val_loss: 1.6470 - val_accuracy: 0.7160\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6341 - accuracy: 0.7057 - val_loss: 1.6473 - val_accuracy: 0.6950\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6256 - accuracy: 0.7080 - val_loss: 1.6326 - val_accuracy: 0.7050\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6176 - accuracy: 0.7065 - val_loss: 1.6277 - val_accuracy: 0.7000\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6100 - accuracy: 0.7064 - val_loss: 1.6165 - val_accuracy: 0.7090\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6018 - accuracy: 0.7095 - val_loss: 1.6094 - val_accuracy: 0.7130\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5946 - accuracy: 0.7083 - val_loss: 1.6021 - val_accuracy: 0.7090\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5871 - accuracy: 0.7091 - val_loss: 1.5955 - val_accuracy: 0.7120\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5798 - accuracy: 0.7083 - val_loss: 1.5896 - val_accuracy: 0.7050\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5725 - accuracy: 0.7095 - val_loss: 1.5814 - val_accuracy: 0.7150\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5655 - accuracy: 0.7085 - val_loss: 1.5764 - val_accuracy: 0.7150\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5582 - accuracy: 0.7101 - val_loss: 1.5706 - val_accuracy: 0.7050\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5515 - accuracy: 0.7103 - val_loss: 1.5613 - val_accuracy: 0.7120\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.5447 - accuracy: 0.7089 - val_loss: 1.5543 - val_accuracy: 0.7110\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5383 - accuracy: 0.7097 - val_loss: 1.5460 - val_accuracy: 0.7120\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5315 - accuracy: 0.7100 - val_loss: 1.5432 - val_accuracy: 0.7110\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5252 - accuracy: 0.7124 - val_loss: 1.5296 - val_accuracy: 0.7220\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.5185 - accuracy: 0.7113 - val_loss: 1.5238 - val_accuracy: 0.7190\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5120 - accuracy: 0.7112 - val_loss: 1.5220 - val_accuracy: 0.7160\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.5063 - accuracy: 0.7115 - val_loss: 1.5175 - val_accuracy: 0.7160\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4999 - accuracy: 0.7143 - val_loss: 1.5159 - val_accuracy: 0.7030\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4935 - accuracy: 0.7128 - val_loss: 1.5022 - val_accuracy: 0.7200\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4875 - accuracy: 0.7125 - val_loss: 1.4967 - val_accuracy: 0.7180\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4811 - accuracy: 0.7135 - val_loss: 1.4911 - val_accuracy: 0.7150\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4752 - accuracy: 0.7144 - val_loss: 1.4819 - val_accuracy: 0.7260\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4699 - accuracy: 0.7140 - val_loss: 1.4774 - val_accuracy: 0.7240\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4637 - accuracy: 0.7167 - val_loss: 1.4788 - val_accuracy: 0.7180\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4581 - accuracy: 0.7157 - val_loss: 1.4678 - val_accuracy: 0.7180\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4529 - accuracy: 0.7147 - val_loss: 1.4624 - val_accuracy: 0.7210\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4468 - accuracy: 0.7153 - val_loss: 1.4581 - val_accuracy: 0.7200\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4420 - accuracy: 0.7168 - val_loss: 1.4487 - val_accuracy: 0.7250\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4363 - accuracy: 0.7163 - val_loss: 1.4451 - val_accuracy: 0.7240\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4311 - accuracy: 0.7180 - val_loss: 1.4403 - val_accuracy: 0.7240\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4255 - accuracy: 0.7183 - val_loss: 1.4380 - val_accuracy: 0.7170\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.4207 - accuracy: 0.7179 - val_loss: 1.4267 - val_accuracy: 0.7250\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4144 - accuracy: 0.7185 - val_loss: 1.4245 - val_accuracy: 0.7190\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4092 - accuracy: 0.7195 - val_loss: 1.4195 - val_accuracy: 0.7240\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.4040 - accuracy: 0.7199 - val_loss: 1.4148 - val_accuracy: 0.7210\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3999 - accuracy: 0.7217 - val_loss: 1.4102 - val_accuracy: 0.7230\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3938 - accuracy: 0.7193 - val_loss: 1.4052 - val_accuracy: 0.7210\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3889 - accuracy: 0.7219 - val_loss: 1.3976 - val_accuracy: 0.7250\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3834 - accuracy: 0.7216 - val_loss: 1.3930 - val_accuracy: 0.7220\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3783 - accuracy: 0.7225 - val_loss: 1.3881 - val_accuracy: 0.7190\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3742 - accuracy: 0.7229 - val_loss: 1.3834 - val_accuracy: 0.7250\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3686 - accuracy: 0.7240 - val_loss: 1.3850 - val_accuracy: 0.7260\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3638 - accuracy: 0.7237 - val_loss: 1.3736 - val_accuracy: 0.7260\n",
      "Epoch 115/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3589 - accuracy: 0.7233 - val_loss: 1.3684 - val_accuracy: 0.7250\n",
      "Epoch 116/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3545 - accuracy: 0.7233 - val_loss: 1.3687 - val_accuracy: 0.7200\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3494 - accuracy: 0.7227 - val_loss: 1.3605 - val_accuracy: 0.7260\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3447 - accuracy: 0.7232 - val_loss: 1.3543 - val_accuracy: 0.7300\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3406 - accuracy: 0.7223 - val_loss: 1.3556 - val_accuracy: 0.7260\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3355 - accuracy: 0.7248 - val_loss: 1.3516 - val_accuracy: 0.7260\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to get and visualize the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-d9f7a74ada55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0macc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL1_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mval_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL1_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer.\n",
    "\n",
    "To complete our comparison, let's use `model.evaluate()` again on the appropriate variables to compare results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 549us/step - loss: 1.3403 - accuracy: 0.7236\n",
      "47/47 [==============================] - 0s 511us/step - loss: 1.3516 - accuracy: 0.7147\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3403023481369019, 0.7235999703407288]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output: [1.3186310468037923, 0.72266666663487755]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.351637840270996, 0.7146666646003723]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [1.3541648308436076, 0.70800000031789145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dropout Regularization\n",
    "\n",
    "Dropout Regularization is accomplished by adding in an additional `Dropout` layer wherever we want to use it, and providing a percentage value for how likely any given neuron is to get \"dropped out\" during this layer. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Dropout` from `keras.layers`\n",
    "* Recreate the same network we have above, but this time without any L1 or L2 regularization\n",
    "* Add a `Dropout` layer between hidden layer 1 and hidden layer 2.  This should have a dropout chance of `0.3`.\n",
    "* Add a `Dropout` layer between hidden layer 2 and the output layer.  This should have a dropout chance of `0.3`.\n",
    "* Compile the model with the exact same hyperparameters as all other models we've built. \n",
    "* Fit the model with the same hyperparameters we've used above.  But this time, train the model for `200` epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 1.9576 - accuracy: 0.1551 - val_loss: 1.9403 - val_accuracy: 0.1490\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.9440 - accuracy: 0.1612 - val_loss: 1.9294 - val_accuracy: 0.1920\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.9325 - accuracy: 0.1848 - val_loss: 1.9200 - val_accuracy: 0.2170\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.9268 - accuracy: 0.1784 - val_loss: 1.9120 - val_accuracy: 0.2180\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.9161 - accuracy: 0.1991 - val_loss: 1.9031 - val_accuracy: 0.2240\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.9069 - accuracy: 0.2129 - val_loss: 1.8937 - val_accuracy: 0.2340\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8973 - accuracy: 0.2180 - val_loss: 1.8847 - val_accuracy: 0.2460\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8907 - accuracy: 0.2237 - val_loss: 1.8737 - val_accuracy: 0.2600\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8812 - accuracy: 0.2335 - val_loss: 1.8615 - val_accuracy: 0.2700\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8685 - accuracy: 0.2339 - val_loss: 1.8479 - val_accuracy: 0.2960\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.8579 - accuracy: 0.2516 - val_loss: 1.8329 - val_accuracy: 0.3110\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.8432 - accuracy: 0.2623 - val_loss: 1.8173 - val_accuracy: 0.3220\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.8299 - accuracy: 0.2652 - val_loss: 1.7993 - val_accuracy: 0.3380\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8165 - accuracy: 0.2736 - val_loss: 1.7811 - val_accuracy: 0.3570\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.8006 - accuracy: 0.2872 - val_loss: 1.7615 - val_accuracy: 0.3730\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7897 - accuracy: 0.2972 - val_loss: 1.7419 - val_accuracy: 0.3910\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7599 - accuracy: 0.3065 - val_loss: 1.7210 - val_accuracy: 0.4180\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7506 - accuracy: 0.3189 - val_loss: 1.6967 - val_accuracy: 0.4300\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.7321 - accuracy: 0.3261 - val_loss: 1.6726 - val_accuracy: 0.4430\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 1.7198 - accuracy: 0.33 - 0s 5ms/step - loss: 1.7202 - accuracy: 0.3324 - val_loss: 1.6518 - val_accuracy: 0.4650\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.6992 - accuracy: 0.3537 - val_loss: 1.6274 - val_accuracy: 0.4720\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6776 - accuracy: 0.3571 - val_loss: 1.6002 - val_accuracy: 0.4880\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6559 - accuracy: 0.3765 - val_loss: 1.5761 - val_accuracy: 0.4960\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6232 - accuracy: 0.3881 - val_loss: 1.5460 - val_accuracy: 0.5150\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.6149 - accuracy: 0.3928 - val_loss: 1.5203 - val_accuracy: 0.5270\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5900 - accuracy: 0.4097 - val_loss: 1.4976 - val_accuracy: 0.5380\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5671 - accuracy: 0.4196 - val_loss: 1.4662 - val_accuracy: 0.5480\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.5422 - accuracy: 0.4244 - val_loss: 1.4415 - val_accuracy: 0.5650\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5233 - accuracy: 0.4397 - val_loss: 1.4136 - val_accuracy: 0.5750\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.5077 - accuracy: 0.4479 - val_loss: 1.3893 - val_accuracy: 0.5850\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.4811 - accuracy: 0.4576 - val_loss: 1.3607 - val_accuracy: 0.6010\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4557 - accuracy: 0.4691 - val_loss: 1.3356 - val_accuracy: 0.6130\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.4372 - accuracy: 0.4740 - val_loss: 1.3156 - val_accuracy: 0.6160\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.4215 - accuracy: 0.4856 - val_loss: 1.2863 - val_accuracy: 0.6370\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3932 - accuracy: 0.4989 - val_loss: 1.2624 - val_accuracy: 0.6440\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.3785 - accuracy: 0.5005 - val_loss: 1.2390 - val_accuracy: 0.6530\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3541 - accuracy: 0.5103 - val_loss: 1.2169 - val_accuracy: 0.6640\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3477 - accuracy: 0.5137 - val_loss: 1.1945 - val_accuracy: 0.6690\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.3256 - accuracy: 0.5244 - val_loss: 1.1746 - val_accuracy: 0.6780\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.3075 - accuracy: 0.5323 - val_loss: 1.1542 - val_accuracy: 0.6850\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2890 - accuracy: 0.5403 - val_loss: 1.1371 - val_accuracy: 0.6860\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2656 - accuracy: 0.5477 - val_loss: 1.1161 - val_accuracy: 0.6950\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2572 - accuracy: 0.5556 - val_loss: 1.0976 - val_accuracy: 0.6990\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2317 - accuracy: 0.5575 - val_loss: 1.0789 - val_accuracy: 0.7000\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.2175 - accuracy: 0.5704 - val_loss: 1.0597 - val_accuracy: 0.7040\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.2103 - accuracy: 0.5633 - val_loss: 1.0437 - val_accuracy: 0.7060\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.1874 - accuracy: 0.5761 - val_loss: 1.0296 - val_accuracy: 0.7110\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1842 - accuracy: 0.5691 - val_loss: 1.0174 - val_accuracy: 0.7090\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1643 - accuracy: 0.5879 - val_loss: 1.0028 - val_accuracy: 0.7160\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1515 - accuracy: 0.5885 - val_loss: 0.9860 - val_accuracy: 0.7130\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1410 - accuracy: 0.5932 - val_loss: 0.9701 - val_accuracy: 0.7150\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.1203 - accuracy: 0.6028 - val_loss: 0.9615 - val_accuracy: 0.7150\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.1067 - accuracy: 0.6057 - val_loss: 0.9491 - val_accuracy: 0.7170\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0906 - accuracy: 0.6112 - val_loss: 0.9322 - val_accuracy: 0.7200\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0821 - accuracy: 0.6128 - val_loss: 0.9242 - val_accuracy: 0.7180\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0826 - accuracy: 0.6060 - val_loss: 0.9130 - val_accuracy: 0.7190\n",
      "Epoch 57/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0572 - accuracy: 0.6197 - val_loss: 0.9007 - val_accuracy: 0.7200\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 7ms/step - loss: 1.0589 - accuracy: 0.6180 - val_loss: 0.8938 - val_accuracy: 0.7240\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.0472 - accuracy: 0.6263 - val_loss: 0.8893 - val_accuracy: 0.7250\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0420 - accuracy: 0.6269 - val_loss: 0.8780 - val_accuracy: 0.7250\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 1.0299 - accuracy: 0.6281 - val_loss: 0.8669 - val_accuracy: 0.7220\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0184 - accuracy: 0.6333 - val_loss: 0.8586 - val_accuracy: 0.7270\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0035 - accuracy: 0.6420 - val_loss: 0.8492 - val_accuracy: 0.7250\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9915 - accuracy: 0.6385 - val_loss: 0.8442 - val_accuracy: 0.7260\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9812 - accuracy: 0.6435 - val_loss: 0.8342 - val_accuracy: 0.7310\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9777 - accuracy: 0.6508 - val_loss: 0.8257 - val_accuracy: 0.7310\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9698 - accuracy: 0.6559 - val_loss: 0.8186 - val_accuracy: 0.7310\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9670 - accuracy: 0.6523 - val_loss: 0.8190 - val_accuracy: 0.7310\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9518 - accuracy: 0.6517 - val_loss: 0.8084 - val_accuracy: 0.7290\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9477 - accuracy: 0.6547 - val_loss: 0.8036 - val_accuracy: 0.7300\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9344 - accuracy: 0.6685 - val_loss: 0.7975 - val_accuracy: 0.7290\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.9379 - accuracy: 0.6557 - val_loss: 0.7933 - val_accuracy: 0.7330\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9179 - accuracy: 0.6679 - val_loss: 0.7854 - val_accuracy: 0.7330\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.9188 - accuracy: 0.6655 - val_loss: 0.7790 - val_accuracy: 0.7340\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8984 - accuracy: 0.6745 - val_loss: 0.7733 - val_accuracy: 0.7340\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.9038 - accuracy: 0.6732 - val_loss: 0.7701 - val_accuracy: 0.7340\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8925 - accuracy: 0.6775 - val_loss: 0.7656 - val_accuracy: 0.7330\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8825 - accuracy: 0.6800 - val_loss: 0.7580 - val_accuracy: 0.7340\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.8859 - accuracy: 0.6771 - val_loss: 0.7573 - val_accuracy: 0.7320\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8829 - accuracy: 0.6800 - val_loss: 0.7540 - val_accuracy: 0.7330\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8667 - accuracy: 0.6833 - val_loss: 0.7487 - val_accuracy: 0.7330\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8684 - accuracy: 0.6875 - val_loss: 0.7464 - val_accuracy: 0.7350\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8534 - accuracy: 0.6873 - val_loss: 0.7394 - val_accuracy: 0.7380\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8446 - accuracy: 0.6907 - val_loss: 0.7355 - val_accuracy: 0.7350\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8452 - accuracy: 0.6923 - val_loss: 0.7341 - val_accuracy: 0.7390\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8473 - accuracy: 0.6935 - val_loss: 0.7314 - val_accuracy: 0.7390\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.8311 - accuracy: 0.6997 - val_loss: 0.7282 - val_accuracy: 0.7390\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8366 - accuracy: 0.6947 - val_loss: 0.7254 - val_accuracy: 0.7420\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8161 - accuracy: 0.7025 - val_loss: 0.7208 - val_accuracy: 0.7400\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8152 - accuracy: 0.6996 - val_loss: 0.7165 - val_accuracy: 0.7390\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.8191 - accuracy: 0.7052 - val_loss: 0.7162 - val_accuracy: 0.7410\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8132 - accuracy: 0.6995 - val_loss: 0.7122 - val_accuracy: 0.7430\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8168 - accuracy: 0.6977 - val_loss: 0.7104 - val_accuracy: 0.7400\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.8137 - accuracy: 0.6979 - val_loss: 0.7111 - val_accuracy: 0.7400\n",
      "Epoch 95/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7935 - accuracy: 0.7067 - val_loss: 0.7043 - val_accuracy: 0.7450\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7935 - accuracy: 0.7100 - val_loss: 0.7059 - val_accuracy: 0.7440\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7958 - accuracy: 0.7035 - val_loss: 0.7022 - val_accuracy: 0.7460\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.7820 - accuracy: 0.7157 - val_loss: 0.6964 - val_accuracy: 0.7440\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7880 - accuracy: 0.7119 - val_loss: 0.6942 - val_accuracy: 0.7460\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7770 - accuracy: 0.7112 - val_loss: 0.6958 - val_accuracy: 0.7450\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7805 - accuracy: 0.7083 - val_loss: 0.6927 - val_accuracy: 0.7450\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7583 - accuracy: 0.7240 - val_loss: 0.6903 - val_accuracy: 0.7470\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7602 - accuracy: 0.7213 - val_loss: 0.6871 - val_accuracy: 0.7450\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7689 - accuracy: 0.7167 - val_loss: 0.6834 - val_accuracy: 0.7500\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.7624 - accuracy: 0.7167 - val_loss: 0.6816 - val_accuracy: 0.7480\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7656 - accuracy: 0.7179 - val_loss: 0.6825 - val_accuracy: 0.7500\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7639 - accuracy: 0.7163 - val_loss: 0.6804 - val_accuracy: 0.7490\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7463 - accuracy: 0.7233 - val_loss: 0.6771 - val_accuracy: 0.7480\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7498 - accuracy: 0.7284 - val_loss: 0.6782 - val_accuracy: 0.7460\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7468 - accuracy: 0.7196 - val_loss: 0.6762 - val_accuracy: 0.7490\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7467 - accuracy: 0.7192 - val_loss: 0.6706 - val_accuracy: 0.7440\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7328 - accuracy: 0.7277 - val_loss: 0.6709 - val_accuracy: 0.7500\n",
      "Epoch 113/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7197 - accuracy: 0.7291 - val_loss: 0.6681 - val_accuracy: 0.7510\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7332 - accuracy: 0.7276 - val_loss: 0.6690 - val_accuracy: 0.7510\n",
      "Epoch 115/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7251 - accuracy: 0.7347 - val_loss: 0.6689 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7163 - accuracy: 0.7324 - val_loss: 0.6651 - val_accuracy: 0.7490\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7197 - accuracy: 0.7327 - val_loss: 0.6636 - val_accuracy: 0.7470\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7150 - accuracy: 0.7357 - val_loss: 0.6618 - val_accuracy: 0.7450\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7104 - accuracy: 0.7396 - val_loss: 0.6623 - val_accuracy: 0.7480\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7155 - accuracy: 0.7297 - val_loss: 0.6627 - val_accuracy: 0.7470\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7114 - accuracy: 0.7375 - val_loss: 0.6623 - val_accuracy: 0.7490\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6997 - accuracy: 0.7408 - val_loss: 0.6584 - val_accuracy: 0.7480\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6975 - accuracy: 0.7375 - val_loss: 0.6584 - val_accuracy: 0.7470\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6994 - accuracy: 0.7427 - val_loss: 0.6557 - val_accuracy: 0.7470\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.7476 - val_loss: 0.6568 - val_accuracy: 0.7500\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.7439 - val_loss: 0.6531 - val_accuracy: 0.7470\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6892 - accuracy: 0.7405 - val_loss: 0.6543 - val_accuracy: 0.7490\n",
      "Epoch 128/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6854 - accuracy: 0.7437 - val_loss: 0.6534 - val_accuracy: 0.7480\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6781 - accuracy: 0.7484 - val_loss: 0.6543 - val_accuracy: 0.7510\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6897 - accuracy: 0.7443 - val_loss: 0.6525 - val_accuracy: 0.7530\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6750 - accuracy: 0.7461 - val_loss: 0.6499 - val_accuracy: 0.7540\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6786 - accuracy: 0.7448 - val_loss: 0.6520 - val_accuracy: 0.7560\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6741 - accuracy: 0.7504 - val_loss: 0.6499 - val_accuracy: 0.7510\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6738 - accuracy: 0.7464 - val_loss: 0.6462 - val_accuracy: 0.7550\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6780 - accuracy: 0.7536 - val_loss: 0.6482 - val_accuracy: 0.7490\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6621 - accuracy: 0.7581 - val_loss: 0.6461 - val_accuracy: 0.7510\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6632 - accuracy: 0.7521 - val_loss: 0.6448 - val_accuracy: 0.7530\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6671 - accuracy: 0.7471 - val_loss: 0.6450 - val_accuracy: 0.7580\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6573 - accuracy: 0.7559 - val_loss: 0.6441 - val_accuracy: 0.7520\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6490 - accuracy: 0.7644 - val_loss: 0.6437 - val_accuracy: 0.7540\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6530 - accuracy: 0.7597 - val_loss: 0.6428 - val_accuracy: 0.7560\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6399 - accuracy: 0.7597 - val_loss: 0.6404 - val_accuracy: 0.7540\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6541 - accuracy: 0.7532 - val_loss: 0.6421 - val_accuracy: 0.7580\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6374 - accuracy: 0.7615 - val_loss: 0.6422 - val_accuracy: 0.7550\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6406 - accuracy: 0.7529 - val_loss: 0.6399 - val_accuracy: 0.7530\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6398 - accuracy: 0.7607 - val_loss: 0.6392 - val_accuracy: 0.7550\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6280 - accuracy: 0.7648 - val_loss: 0.6388 - val_accuracy: 0.7570\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6366 - accuracy: 0.7623 - val_loss: 0.6372 - val_accuracy: 0.7580\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6324 - accuracy: 0.7631 - val_loss: 0.6352 - val_accuracy: 0.7610\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6306 - accuracy: 0.7640 - val_loss: 0.6383 - val_accuracy: 0.7560\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6275 - accuracy: 0.7644 - val_loss: 0.6376 - val_accuracy: 0.7590\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6280 - accuracy: 0.7653 - val_loss: 0.6355 - val_accuracy: 0.7560\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6280 - accuracy: 0.7648 - val_loss: 0.6335 - val_accuracy: 0.7580\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6320 - accuracy: 0.7649 - val_loss: 0.6341 - val_accuracy: 0.7600\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6214 - accuracy: 0.7661 - val_loss: 0.6355 - val_accuracy: 0.7560\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6260 - accuracy: 0.7677 - val_loss: 0.6357 - val_accuracy: 0.7590\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6233 - accuracy: 0.7680 - val_loss: 0.6320 - val_accuracy: 0.7610\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6076 - accuracy: 0.7679 - val_loss: 0.6345 - val_accuracy: 0.7580\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6202 - accuracy: 0.7697 - val_loss: 0.6303 - val_accuracy: 0.7580\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6159 - accuracy: 0.7692 - val_loss: 0.6336 - val_accuracy: 0.7580\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6089 - accuracy: 0.7735 - val_loss: 0.6313 - val_accuracy: 0.7600\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6039 - accuracy: 0.7744 - val_loss: 0.6292 - val_accuracy: 0.7570\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6088 - accuracy: 0.7711 - val_loss: 0.6307 - val_accuracy: 0.7590\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5946 - accuracy: 0.7737 - val_loss: 0.6285 - val_accuracy: 0.7610\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5949 - accuracy: 0.7811 - val_loss: 0.6277 - val_accuracy: 0.7570\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5895 - accuracy: 0.7801 - val_loss: 0.6285 - val_accuracy: 0.7570\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5868 - accuracy: 0.7827 - val_loss: 0.6267 - val_accuracy: 0.7590\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5962 - accuracy: 0.7775 - val_loss: 0.6282 - val_accuracy: 0.7580\n",
      "Epoch 169/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5918 - accuracy: 0.7785 - val_loss: 0.6274 - val_accuracy: 0.7600\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5989 - accuracy: 0.7779 - val_loss: 0.6265 - val_accuracy: 0.7540\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5879 - accuracy: 0.7771 - val_loss: 0.6288 - val_accuracy: 0.7580\n",
      "Epoch 172/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5800 - accuracy: 0.7868 - val_loss: 0.6282 - val_accuracy: 0.7590\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5832 - accuracy: 0.7785 - val_loss: 0.6244 - val_accuracy: 0.7590\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5890 - accuracy: 0.7780 - val_loss: 0.6259 - val_accuracy: 0.7620\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5893 - accuracy: 0.7785 - val_loss: 0.6259 - val_accuracy: 0.7630\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5803 - accuracy: 0.7828 - val_loss: 0.6247 - val_accuracy: 0.7580\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5771 - accuracy: 0.7808 - val_loss: 0.6246 - val_accuracy: 0.7600\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5705 - accuracy: 0.7896 - val_loss: 0.6249 - val_accuracy: 0.7600\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5832 - accuracy: 0.7803 - val_loss: 0.6242 - val_accuracy: 0.7570\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5716 - accuracy: 0.7849 - val_loss: 0.6264 - val_accuracy: 0.7630\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5648 - accuracy: 0.7889 - val_loss: 0.6222 - val_accuracy: 0.7620\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.5696 - accuracy: 0.7907 - val_loss: 0.6268 - val_accuracy: 0.7620\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5761 - accuracy: 0.7827 - val_loss: 0.6249 - val_accuracy: 0.7600\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5572 - accuracy: 0.7873 - val_loss: 0.6234 - val_accuracy: 0.7560\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5580 - accuracy: 0.7907 - val_loss: 0.6240 - val_accuracy: 0.7670\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5639 - accuracy: 0.7861 - val_loss: 0.6224 - val_accuracy: 0.7620\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5701 - accuracy: 0.7853 - val_loss: 0.6224 - val_accuracy: 0.7610\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5506 - accuracy: 0.7897 - val_loss: 0.6222 - val_accuracy: 0.7580\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5495 - accuracy: 0.7981 - val_loss: 0.6219 - val_accuracy: 0.7590\n",
      "Epoch 190/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5614 - accuracy: 0.7848 - val_loss: 0.6207 - val_accuracy: 0.7570\n",
      "Epoch 191/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5422 - accuracy: 0.7968 - val_loss: 0.6209 - val_accuracy: 0.7590\n",
      "Epoch 192/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5483 - accuracy: 0.7939 - val_loss: 0.6215 - val_accuracy: 0.7620\n",
      "Epoch 193/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5374 - accuracy: 0.7988 - val_loss: 0.6210 - val_accuracy: 0.7600\n",
      "Epoch 194/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5482 - accuracy: 0.7904 - val_loss: 0.6212 - val_accuracy: 0.7610\n",
      "Epoch 195/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5406 - accuracy: 0.7943 - val_loss: 0.6210 - val_accuracy: 0.7590\n",
      "Epoch 196/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5417 - accuracy: 0.8003 - val_loss: 0.6208 - val_accuracy: 0.7620\n",
      "Epoch 197/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5412 - accuracy: 0.7960 - val_loss: 0.6214 - val_accuracy: 0.7620\n",
      "Epoch 198/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5385 - accuracy: 0.7993 - val_loss: 0.6193 - val_accuracy: 0.7590\n",
      "Epoch 199/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.5407 - accuracy: 0.7993 - val_loss: 0.6201 - val_accuracy: 0.7620\n",
      "Epoch 200/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5414 - accuracy: 0.8012 - val_loss: 0.6204 - val_accuracy: 0.7580\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the results from `model.evaluate` to see how this change has affected our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/235 [..............................] - ETA: 0s - loss: 0.2735 - accuracy: 0.9062WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "235/235 [==============================] - 0s 605us/step - loss: 0.3658 - accuracy: 0.8732\n",
      "47/47 [==============================] - 0s 574us/step - loss: 0.6117 - accuracy: 0.7667\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36577847599983215, 0.873199999332428]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.36925017188787462, 0.88026666666666664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6117160320281982, 0.7666666507720947]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.69210424280166627, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! However, the variance did become higher again, compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  More Training Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to high variance is to just get more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets.\n",
    "\n",
    "Run the cell below to preprocess our entire dataset, instead of just working with a subset of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the first model that we built, without any regularization or dropout layers included. \n",
    "\n",
    "Train this model for 120 epochs.  All other hyperparameters should stay the same.  Store the fitted model inside of `moredata_model`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, let's check the results returned from `model.evaluate()` to see how this model stacks up with the other techniques we've used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train # Expected Output:  [0.31160746300942971, 0.89160606060606062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test # Expected Output: [0.56076071488857271, 0.8145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.1%. Our test set accuracy went up from ~75% to a staggering 81.45% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
